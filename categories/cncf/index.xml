<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>CNCF on Morgoth</title>
        <link>https://canuxcheng.com/categories/cncf/</link>
        <description>Recent content in CNCF on Morgoth</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Fri, 04 Aug 2023 20:03:39 +0800</lastBuildDate><atom:link href="https://canuxcheng.com/categories/cncf/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>API Gateway</title>
        <link>https://canuxcheng.com/post/k8s_apigateway/</link>
        <pubDate>Fri, 04 Aug 2023 20:03:39 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_apigateway/</guid>
        <description>&lt;h1 id=&#34;api-gateway&#34;&gt;API Gateway&lt;/h1&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes-sigs/gateway-api&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes-sigs/gateway-api&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GatewayClass没有namespace&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;emissary ingress&lt;/li&gt;
&lt;li&gt;kong&lt;/li&gt;
&lt;li&gt;higress&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Application</title>
        <link>https://canuxcheng.com/post/cncf_application/</link>
        <pubDate>Sat, 04 Dec 2021 20:03:39 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/cncf_application/</guid>
        <description>&lt;h1 id=&#34;application-definition--image-build&#34;&gt;Application Definition &amp;amp; Image Build&lt;/h1&gt;
&lt;p&gt;application choreography.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;helm&lt;/li&gt;
&lt;li&gt;backstage&lt;/li&gt;
&lt;li&gt;buildpack.io&lt;/li&gt;
&lt;li&gt;operatorframework&lt;/li&gt;
&lt;li&gt;dapr&lt;/li&gt;
&lt;li&gt;kubevela&lt;/li&gt;
&lt;li&gt;kubevirt&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;backstage&#34;&gt;backstage&lt;/h2&gt;
</description>
        </item>
        <item>
        <title>KubeVirt</title>
        <link>https://canuxcheng.com/post/k8s_kubevirt/</link>
        <pubDate>Mon, 10 May 2021 20:55:52 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_kubevirt/</guid>
        <description>&lt;h1 id=&#34;kubevirt&#34;&gt;KubeVirt&lt;/h1&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubevirt.io/quickstart_cloud/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubevirt.io/quickstart_cloud/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://quay.io/organization/kubevirt&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://quay.io/organization/kubevirt&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.fedoraproject.org/en-US/quick-docs/using-nested-virtualization-in-kvm/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://docs.fedoraproject.org/en-US/quick-docs/using-nested-virtualization-in-kvm/index.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;virtctl&#34;&gt;virtctl&lt;/h2&gt;
&lt;h2 id=&#34;vm&#34;&gt;VM&lt;/h2&gt;
&lt;p&gt;创建vm&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
name: testvm
spec:
running: false
template:
    metadata:
    labels:
        kubevirt.io/size: small
        kubevirt.io/domain: testvm
    spec:
    domain:
        devices:
        disks:
            - name: containerdisk
            disk:
                bus: virtio
            - name: cloudinitdisk
            disk:
                bus: virtio
        interfaces:
        - name: default
            masquerade: {}
        resources:
        requests:
            memory: 64M
    networks:
    - name: default
        pod: {}
    volumes:
        - name: containerdisk
        containerDisk:
            image: quay.io/kubevirt/cirros-container-disk-demo
        - name: cloudinitdisk
        cloudInitNoCloud:
            userDataBase64: SGkuXG4=
    nodeSelector:
        kubernetes.io/arch: arm64
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>AWS Containers</title>
        <link>https://canuxcheng.com/post/aws_containers/</link>
        <pubDate>Fri, 23 Apr 2021 22:02:31 +0000</pubDate>
        
        <guid>https://canuxcheng.com/post/aws_containers/</guid>
        <description>&lt;h1 id=&#34;aws-containers&#34;&gt;AWS Containers&lt;/h1&gt;
&lt;h2 id=&#34;ecr&#34;&gt;ECR&lt;/h2&gt;
&lt;p&gt;Elastic Container Registry.&lt;/p&gt;
&lt;h2 id=&#34;ecs-anywhere&#34;&gt;ECS Anywhere&lt;/h2&gt;
&lt;h2 id=&#34;ecs&#34;&gt;ECS&lt;/h2&gt;
&lt;p&gt;Elastic Container Service.&lt;/p&gt;
&lt;h2 id=&#34;eks-distro&#34;&gt;EKS Distro&lt;/h2&gt;
&lt;h2 id=&#34;eks-anywhere&#34;&gt;EKS Anywhere&lt;/h2&gt;
&lt;h2 id=&#34;eks&#34;&gt;EKS&lt;/h2&gt;
&lt;p&gt;Elastic Kubernetes Service.&lt;/p&gt;
&lt;h3 id=&#34;通过aws-cli创建eks&#34;&gt;通过AWS CLI创建EKS&lt;/h3&gt;
&lt;p&gt;通过MC创建的资源都可以通过CLI(aws)创建.&lt;/p&gt;
&lt;p&gt;创建具有公有和私有子网且符合 Amazon EKS 要求的 Amazon VPC&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ aws cloudformation create-stack \
--stack-name my-eks-vpc-stack \
--region region-code \
--template-url https://amazon-eks.s3.us-west-2.amazonaws.com/cloudformation/2020-10-29/amazon-eks-vpc-private-subnets.yaml 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建集群 IAM 角色并向其附加所需的 Amazon EKS IAM 托管策略&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 创建EKS IAM role
aws iam create-role \
--role-name my-EKSClusterRole \
--assume-role-policy-document file://cluster-role-trust-policy.json&amp;quot; \
--permissions-boundary arn:aws:iam::&amp;lt;Your AWS ID&amp;gt;:policy/ProjAdminsPermBoundaryv2

{
    &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;,
    &amp;quot;Statement&amp;quot;: [
        {
            &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;,
            &amp;quot;Principal&amp;quot;: {
                &amp;quot;Service&amp;quot;: &amp;quot;eks.amazonaws.com&amp;quot;
            },
            &amp;quot;Action&amp;quot;: &amp;quot;sts:AssumeRole&amp;quot;,
            &amp;quot;Condition&amp;quot;: {}
        }
    ]
}

// 绑定role和eks策略
aws iam attach-role-policy \
--policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy \
--role-name my-EKSClusterRole
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建EKS cluster:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aws eks create-cluster --name my-cluster \
--role-arn arn:aws:iam::&amp;lt;ID&amp;gt;:role/my-EKSClusterRole \
--resources-vpc-config vpc.json / --resources-vpc-config subnetIds=subnet-6782e71e,subnet-e7e761ac,securityGroupIds=sg-6979fe18 \
--kubernetes-network-config eks.json / --kubernetes-network-config serviceIpv4Cidr=string,ipFamily=string \
--kubernetes-version &amp;lt;version&amp;gt; --tags &amp;lt;tags&amp;gt; --logging &amp;lt;logging&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;给集群创建节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 创建节点IAM role
aws iam create-role \
--role-name my-EKSNodeRole \
--assume-role-policy-document file://node-role-trust-policy.json&amp;quot; \
--permissions-boundary arn:aws:iam::&amp;lt;ID&amp;gt;:policy/ProjAdminsPermBoundaryv2

{
    &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;,
    &amp;quot;Statement&amp;quot;: [
        {
            &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;,
            &amp;quot;Principal&amp;quot;: {
                &amp;quot;Service&amp;quot;: &amp;quot;ec2.amazonaws.com&amp;quot;
            },
            &amp;quot;Action&amp;quot;: &amp;quot;sts:AssumeRole&amp;quot;,
            &amp;quot;Condition&amp;quot;: {}
        }
    ]
}

// 绑定role和node策略
aws iam attach-role-policy \
--policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy \
--role-name my-EKSNodeRole 

aws iam attach-role-policy \
--policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly \
--role-name my-EKSNodeRole 

aws iam attach-role-policy \
--policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy \
--role-name my-EKSNodeRole 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建managed node group:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aws eks create-nodegroup \
--cluster-name my-cluster --nodegroup-name my-mng  --subnets &amp;lt;value&amp;gt; --node-role &amp;lt;value&amp;gt; \
--scaling-config minSize=integer,maxSize=integer,desiredSize=integer \
--instance-types &amp;lt;value&amp;gt; --ami-type &amp;lt;value&amp;gt; --remote-access &amp;lt;value&amp;gt; --disk-size &amp;lt;value&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将计算机配置为与您的集群通信&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aws eks describe-cluster --name my-cluster
// 通过aws eks自动更新kube.config文件.
// 需要该role具有可以操作eks cluster的policy.
$ aws eks update-kubeconfig --name my-cluster --role-arn &amp;lt;role&amp;gt; --region eu-west-1 --verbose

$ kubectl get svc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;删除集群和节点:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aws eks delete-nodegroup --nodegroup-name my-mng --cluster-name my-cluster
aws eks delete-cluster --name my-cluster
aws cloudformation delete-stack --stack-name my-stack
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;通过eksctl创建eks&#34;&gt;通过eksctl创建EKS&lt;/h3&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/weaveworks/eksctl&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/weaveworks/eksctl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://eksctl.io/usage/schema/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://eksctl.io/usage/schema/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;创建集群和节点：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ eksctl create cluster -f/--config-file ./cluster.yaml
$ eksctl create nodegroup -f/--config-file ./nodegroup.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;删除集群和节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ eksctl delete cluster --name my-cluster --region us-west-2
$ eksctl delete nodegroup --cluster my-cluster --region us-west-2 --name my-ng
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;通过terraform创建eks&#34;&gt;通过Terraform创建EKS&lt;/h3&gt;
</description>
        </item>
        <item>
        <title>K8S CNI</title>
        <link>https://canuxcheng.com/post/k8s_cni/</link>
        <pubDate>Thu, 26 Mar 2020 20:55:52 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_cni/</guid>
        <description>&lt;h1 id=&#34;network-add-ons&#34;&gt;Network add-ons&lt;/h1&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/containernetworking&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/containernetworking&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;flannel&lt;/li&gt;
&lt;li&gt;cilium&lt;/li&gt;
&lt;li&gt;calico&lt;/li&gt;
&lt;li&gt;vpc-cni (aws)&lt;/li&gt;
&lt;li&gt;kube-router&lt;/li&gt;
&lt;li&gt;weavenet&lt;/li&gt;
&lt;li&gt;antrea&lt;/li&gt;
&lt;li&gt;romana&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cilium&#34;&gt;cilium&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/cilium/cilium&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/cilium/cilium&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;flannel&#34;&gt;Flannel&lt;/h2&gt;
&lt;p&gt;flannel是k8s最常用的网络插件.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/coreos/flannel&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/coreos/flannel&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在所有node上部署cni-plugin:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/containernetworking/plugins/releases&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/containernetworking/plugins/releases&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo mkdir -p /opt/cni/bin
// 下载并解压所有插件命令到该目录.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;network-addon(master上操作即可):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;veryfy:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl get nodes
$ kubectl get pod --all-namespaces
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;删除插件:&lt;/p&gt;
&lt;p&gt;删除插件会影响已经部署的pod.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 删除flannel 
$ kubectl delete -f X.yml  
$ sudo systemctl stop kubelet docker

// 第二步，在node节点清理flannel网络留下的文件
ifconfig cni0 down
ip link delete cni0 
ifconfig flannel.1 down
ip link delete flannel.1 
rm -rf /var/lib/cni /etc/cni /run/flannel
$ sudo rm -rf /var/lib/kubelet /var/lib/etcd

// 重启kubelet
$ sudo systemctl start kubelet docker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;修改配置:&lt;/p&gt;
&lt;p&gt;/etc/kube-flannel/net-conf.json&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;Network&amp;quot;: &amp;quot;10.244.0.0/16&amp;quot;,
  &amp;quot;SubnetLen&amp;quot;: 24,
  &amp;quot;SubnetMin&amp;quot;: &amp;quot;10.244.0.0&amp;quot;,
  &amp;quot;SubnetMax&amp;quot;: &amp;quot;10.244.255.0&amp;quot;,
  &amp;quot;Backend&amp;quot;: {
    &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使用kubeadm：&lt;/p&gt;
&lt;p&gt;kubeadm init必须指定flannel的Network参数:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--pod-network-cidr=10.244.0.0/16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果需要修改其它参数，同时需要修改kubeadm的配置&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>K8S CSI</title>
        <link>https://canuxcheng.com/post/k8s_csi/</link>
        <pubDate>Wed, 25 Mar 2020 20:55:52 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_csi/</guid>
        <description>&lt;h1 id=&#34;csi&#34;&gt;CSI&lt;/h1&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/container-storage-interface/spec&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/container-storage-interface/spec&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;rook&lt;/li&gt;
&lt;li&gt;cubefs&lt;/li&gt;
&lt;li&gt;longhorn&lt;/li&gt;
&lt;li&gt;ceph&lt;/li&gt;
&lt;li&gt;minio&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;卷volume&#34;&gt;卷Volume&lt;/h2&gt;
&lt;p&gt;和docker中的一样。&lt;/p&gt;
&lt;p&gt;volume支持的卷类型有: awsEBS, azureDisk, azureFile, gcePD, secret, configMap, emptyDir, hostPath, local, nfs等.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: test-ebs
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-ebs
      name: test-volume
  volumes:
  - name: test-volume
    # 此 AWS EBS 卷必须已经存在
    awsElasticBlockStore:
      volumeID: &amp;quot;&amp;lt;volume-id&amp;gt;&amp;quot;
      fsType: ext4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;AWS的EBS和EFS需要安装驱动:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes-sigs/aws-ebs-csi-driver&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes-sigs/aws-ebs-csi-driver&lt;/a&gt;
&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes-sigs/aws-efs-csi-driver&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes-sigs/aws-efs-csi-driver&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;存储类storageclass&#34;&gt;存储类StorageClass&lt;/h2&gt;
&lt;p&gt;storageclass没有namespace.&lt;/p&gt;
&lt;p&gt;每个存储类包含provisioner, parameters和reclaimPolicy.&lt;/p&gt;
&lt;p&gt;内置provisioner的卷插件:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;awsEBS&lt;/li&gt;
&lt;li&gt;azureFile&lt;/li&gt;
&lt;li&gt;azureDisk&lt;/li&gt;
&lt;li&gt;gcePD&lt;/li&gt;
&lt;li&gt;openstack cinder&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;没有provisioner的卷类型可以使用外部插件或者自己开发.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;awsEBS:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: sourcegraph
labels:
  deploy: sourcegraph
# provisioner: ebs.csi.aws.com
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2 # This configures SSDs (default).
  fsType: ext4 # (default)
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;nfs:&lt;/p&gt;
&lt;p&gt;kubernetes不包含nfs驱动，需要使用外部驱动创建nfs存储类.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: example-nfs
provisioner: example.com/external-nfs
parameters:
  server: nfs-server.example.com
  path: /share
  readOnly: false
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;local:&lt;/p&gt;
&lt;p&gt;本地卷不支持动态制备.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;持久卷pv&#34;&gt;持久卷PV&lt;/h2&gt;
&lt;p&gt;persistentvolume没有namespace, 用来指定具体的存储资源。有静态和动态两种方式，最终需要绑定到pvc上。&lt;/p&gt;
&lt;p&gt;pv的回收策略ReclaimPolicy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Retained保留&lt;/li&gt;
&lt;li&gt;Deleted删除&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;pv的卷绑定模式volumeBindingMode:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;WaitForFirstConsumer&lt;/li&gt;
&lt;li&gt;Immediate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;卷模式volumeMode:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Filesystem(默认)&lt;/li&gt;
&lt;li&gt;Block&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;访问模式accessMode:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RWO: ReadWriteOnce&lt;/li&gt;
&lt;li&gt;ROX: ReadOnlyMany&lt;/li&gt;
&lt;li&gt;RWX: ReadWriteMany&lt;/li&gt;
&lt;li&gt;RWOP: ReadWriteOncePod&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;卷的阶段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Avaliable&lt;/li&gt;
&lt;li&gt;Bound&lt;/li&gt;
&lt;li&gt;Released&lt;/li&gt;
&lt;li&gt;Failed&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;static-volume-provisioning&#34;&gt;static volume provisioning&lt;/h3&gt;
&lt;p&gt;静态pvc和pv的绑定通过storageClassName, accessMode和capacity来判断。&lt;/p&gt;
&lt;p&gt;pv中的capacity必须大于等于pvc。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: nas-csi-pv
  labels:
    app: demo
spec:
  storageClassName: 
  persistentVolumeReclaimPolicy: Retained/Recycled
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 5Gi
  hostPath:
    path: &amp;quot;/home/path&amp;quot;
  csi:
    driver: ...
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dynamic-volume-provisioning&#34;&gt;dynamic volume provisioning&lt;/h3&gt;
&lt;p&gt;动态pv需要storageclass, 由StorageClass动态的创建PV, 不需要手动创建pv，只需要在pvc中指定storageclass即可.&lt;/p&gt;
&lt;p&gt;storageclass没有namespace&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: sourcegraph
labels:
  deploy: sourcegraph
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2 
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pgsql
  labels:
    deploy: sourcegraph
    sourcegraph-resource-requires: no-cluster-admin
    app.kubernetes.io/component: pgsql
spec:
  // 通过storageClassName自动给pvc创建pv
  storageClassName: sourcegraph
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
    storage: 200Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pvc&#34;&gt;PVC&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nsa-pvc
  namespace: test
labels:
  app: demo
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 5Gi 
  // 通过selector让PVC使用指定的PV。
  selector:
    app: demo
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;volumesnapshotclass&#34;&gt;VolumeSnapshotClass&lt;/h2&gt;
&lt;h2 id=&#34;volumesnapshot&#34;&gt;VolumeSnapshot&lt;/h2&gt;
&lt;p&gt;VS是对资源的请求.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: new-snapshot-test
spec:
  volumeSnapshotClassName: csi-hostpath-snapclass
  source:
    persistentVolumeClaimName: pvc-test
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;volumesnapshotcontent&#34;&gt;VolumeSnapshotContent&lt;/h2&gt;
&lt;p&gt;VSC实际中资源管理.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotContent
metadata:
  name: snapcontent-72d9a349-aacd-42d2-a240-d775650d2455
spec:
  deletionPolicy: Delete
  driver: hostpath.csi.k8s.io
  source:
    volumeHandle: ee0cfb94-f8d4-11e9-b2d8-0242ac110002
  volumeSnapshotClassName: csi-hostpath-snapclass
  volumeSnapshotRef:
    name: new-snapshot-test
    namespace: default
    uid: 72d9a349-aacd-42d2-a240-d775650d2455
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>Kubectl</title>
        <link>https://canuxcheng.com/post/k8s_kubectl/</link>
        <pubDate>Fri, 10 Jan 2020 20:58:01 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_kubectl/</guid>
        <description>&lt;h1 id=&#34;kubectl&#34;&gt;kubectl&lt;/h1&gt;
&lt;p&gt;kubectl是kubernetes的管理工具.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.io/docs/tasks/tools/#kubectl&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.io/docs/tasks/tools/#kubectl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/cloudnativelabs/kube-shell&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/cloudnativelabs/kube-shell&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/jonmosco/kube-ps1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/jonmosco/kube-ps1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/ahmetb/kubectx&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ahmetb/kubectx&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在master上通过kubectl命令管理集群.&lt;/p&gt;
&lt;p&gt;kubectl 版本和集群版本之间的差异必须在一个小版本号内。 例如：v1.24 版本的客户端能与 v1.23、 v1.24 和 v1.25 版本的控制面通信。 用最新兼容版的 kubectl 有助于避免不可预见的问题。&lt;/p&gt;
&lt;h2 id=&#34;options&#34;&gt;Options&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;kubectl options # 查看所有命令可用选项

--kubeconfig
kubectl --kubeconfig=$HOME/.kube.config (default)

-n/--namespace
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;basic-command&#34;&gt;basic command&lt;/h2&gt;
&lt;p&gt;create:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 通过yaml或json文件创建资源
$ kubectl create -f FILENAME [options]

options:
-f/--filename

kubectl create secret tls kubernetes-dashboard-tls --key ca.key --cert ca.crt -n kubernetes-dashboard

// 输出一个资源的yaml格式.
kubectl create deployment &amp;lt;name&amp;gt; --image=&amp;lt;img-name&amp;gt; --dry-run=client --output=yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;delete:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 删除资源
$ kubectl delete (-f FILENAME | -k DICT | TYPE [(NAME|-l label|--all)]) [optiions]

options:
-f/--filename
--all  
--all-namespaces
--force

$ kubectl delete pods --all
$ kubectl delete pod &amp;lt;name&amp;gt;
// 删除指定ns下所有资源.
$ kubectl delete all --all -n {namespace}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;删除带有finalizers字段的对象，对象实际被更新了，没有真的被删除。可以通过patch来删除。&lt;/p&gt;
&lt;p&gt;删除带有ownerReferences字段的对象，删除父对象默认会删除所有子对象，通过&amp;ndash;cascade=false只删除父对象。&lt;/p&gt;
&lt;p&gt;expose:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 创建serivce:
$ kubectl expose (-f FILENAME | TYPE NAME) [--port=port] [--protocol=TCP|UDP|SCTP] [--target-port=...] [--name=...] [--external-ip=...] [--type=type] [options]

# 创建service，并且使用NodePort
# nodeport可以使用任意节点的IP访问.
$ kubectl expose deployment hello-world --type=NodePort --name=example-service

// 获取service的yaml
kubectl expose deployment &amp;lt;deploy-name&amp;gt; --type=LoadBalancer --port=80 --dry-run=client --output=yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 创建pod/container(docker run):
$ kubectl run NAME --image=image [--env=&amp;quot;key=value&amp;quot;] [--port=port] [--replicas=replicas] [--dry-run=bool] [--overrides=inline-json] [--command] -- [COMMAND] [args...] [options]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;set:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl set SUBCOMMAND [options]

// 更新镜像
$ kubectl set image deployment.v1.apps/&amp;lt;deploy-name&amp;gt; &amp;lt;container-name&amp;gt;=&amp;lt;image:tag&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;get:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 获取resource信息(docker ps)
$ kubectl [-n &amp;lt;namespace&amp;gt;] get [resource] [flags] [options]

options:
-A/--all-namespaces
-f/--filename
-o/--output json/yaml/json/wide/name/...
-w/--watch
--watch-only

kubectl get nodes --show-labels

kubectl get nodes/no # 获取node节点信息
kubectl get namespace/ns # 获取namespace信息
kubectl get componentstatuses/cs

kubectl get all --all-namespaces
kubectl -n kube-system get all 

kubectl get pod/pods/po 
kubectl get po -w/--watch
kubectl get po -o wide

kubectl get deployment/deployments/deploy

kubectl get service/services/svc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;explain:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 查看资源解析
$ kubectl explain RESOURCE [options]

kubectl explain deploy.spec
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;edit:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 在线修改资源，直接生效.
$ kubectl edit
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;advanced-commands&#34;&gt;advanced commands&lt;/h2&gt;
&lt;p&gt;apply:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl apply
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;diff:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl diff
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;patch&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl patch

// 删除terminating状态的resource
$ kubectl patch  persistentvolumeclaim/storage -p &#39;{&amp;quot;metadata&amp;quot;:{&amp;quot;finalizers&amp;quot;:null}}&#39; -n &amp;lt;ns&amp;gt;

$ kubectl replace

$ kubectl wait

$ kubectl kustomize
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;deploy-command&#34;&gt;deploy command&lt;/h2&gt;
&lt;p&gt;scale:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl scale
$ kubectl scale deployment &amp;lt;deploy-name&amp;gt; -n &amp;lt;ns&amp;gt; --replicas=0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;autoscale:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl autoscale
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;rollout:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 回滚
$ kubectl rollout

// 暂停/恢复/重启 deployment.
kubectl rollout pause/resume/restart  deploy/katib-mysql -n kubeflow
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;debugtroubleshoot&#34;&gt;debug&amp;amp;troubleshoot&lt;/h2&gt;
&lt;p&gt;describe:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// like docker inspect
$ kubectl describe (-f FILENAME | TYPE ... | TYPE/NAME) [options]

options:
-A/--all-namespaces
-f/--filename

// 查看node信息
$ kubectl describe nodes &amp;lt;name&amp;gt;

//查看pod详细信息
$ kubectl describe pods &amp;lt;name&amp;gt; 

// 获取k8s-dashboard的token
$ kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk &#39;{print $1}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;logs:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# like docker logs
$ kubectl logs [-f] [-p] (POD | TYPE/NAME) [-c CONTAINER] [options]

options:
--all-containers
-f/--follow
-p/--previous

// 查看pod中指定container log
kubectl logs -f &amp;lt;pod&amp;gt; -c &amp;lt;container&amp;gt; -n &amp;lt;ns&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;exec:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# like docker exec
$ kubectl exec (POD | TYPE/NAME) [-c CONTAINER] [flags] -- COMMAND [args...] [options]

options:
-c/--container
-i/--stdin
-t/--tty

$ kubectl exec -it &amp;lt;pod&amp;gt; -n &amp;lt;ns&amp;gt; -- /bin/bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;port-forward:&lt;/p&gt;
&lt;p&gt;由于已知的限制，目前的端口转发仅适用于 TCP 协议.&lt;/p&gt;
&lt;p&gt;只能通过运行转发命令的IP/FQDN访问。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl port-forward

// 转发本地端口到deploy/rs/svc/pod
$ kubectl port-forward svc/redis-service 6379:6379 -n redis

$ kubectl port-forward --address 0.0.0.0 -n kubernetes-dashboard service/kubernetes-dashboard 8080:443
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;proxy:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl proxy

// 通过proxy可以访问默认的clusterIP服务.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;attach:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl attach
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;cp:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl cp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;auth:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl auth
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;debug:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl debug

// debug node -- 直接进入node的shell，log在/host/var/log
kubectl debug node/&amp;lt;name&amp;gt;  -it   --image=ubuntu
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;cluster-management&#34;&gt;cluster management&lt;/h2&gt;
&lt;p&gt;top:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl top

$ kubectl cluster-info

$ kubectl certificate

// 节点变成unschedulable
$ kubectl cordon

// 节点变成schedulable
$ kubectl uncordon

// 安全下线节点：Ready,SchedulingDisabled
$ kubectl drain
kubectl drain &amp;lt;node&amp;gt; --delete-emptydir-data --ignore-daemonsets

//定义污点
$ kubectl taint
kubectl taint nodes &amp;lt;node&amp;gt; &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;:NoSchedule-
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;settings-commands&#34;&gt;Settings Commands&lt;/h2&gt;
&lt;p&gt;label&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;label          Update the labels on a resource

kubectl label nodes &amp;lt;your-node-name&amp;gt; disktype=ssd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;annotate:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;annotate       Update the annotations on a resource
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;completion:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;completion     Output shell completion code for the specified shell (bash or zsh)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;others&#34;&gt;others&lt;/h2&gt;
&lt;p&gt;api-resources:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// api-resources  Print the supported API resources on the server.
kubectl api-resources
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;api-versions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// api-versions   Print the supported API versions on the server, in the form of &amp;quot;group/version&amp;quot;.
kubectl api-versions
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;config:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 参考kubectx
config         Modify kubeconfig files.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;plugin:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 参考krew
plugin         Provides utilities for interacting with plugins.

// 查看安装的plugin
$ kubectl plugin list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;version:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;version        Print the client and server version information.
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;shell-auotcompletion&#34;&gt;shell-auotcompletion&lt;/h2&gt;
&lt;p&gt;bash/zsh 自动补全工具.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/#enabling-shell-autocompletion&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.io/docs/tasks/tools/install-kubectl/#enabling-shell-autocompletion&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;krew&#34;&gt;krew&lt;/h1&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes-sigs/krew&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes-sigs/krew&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;安装kubectl plugin的工具&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 下载插件列表
$ kubectl krew update

// 查找插件
$ kubectl krew search

// 安装插件
$ kubectl krew install &amp;lt;name&amp;gt;

// 升级插件
$ kubectl krew upgrade

// 卸载插件
$ kubectl krew uninstall &amp;lt;name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;kubectx&#34;&gt;kubectx&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/ahmetb/kubectx&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ahmetb/kubectx&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在cluster和namespace之间切换的命令行工具.&lt;/p&gt;
&lt;p&gt;包括kubectx 和 kubedns&lt;/p&gt;
&lt;p&gt;通过krew安装，作为kubectl的plugin:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl krew install ctx
$ kubectl krew install ns
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;circtl&#34;&gt;circtl&lt;/h1&gt;
&lt;p&gt;用于对node调试.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes-sigs/cri-tools&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes-sigs/cri-tools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;nerdctl&#34;&gt;nerdctl&lt;/h2&gt;
&lt;p&gt;调试node。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/containerd/nerdctl&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/containerd/nerdctl&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;telepresence&#34;&gt;telepresence&lt;/h2&gt;
&lt;p&gt;用于本地调试集群上的服务.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/telepresenceio/telepresence&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/telepresenceio/telepresence&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Contribution</title>
        <link>https://canuxcheng.com/post/k8s_oss/</link>
        <pubDate>Fri, 10 Jan 2020 20:55:52 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_oss/</guid>
        <description>&lt;h1 id=&#34;kubernetes-contribution&#34;&gt;Kubernetes Contribution&lt;/h1&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/community/blob/master/contributors/guide/contributor-cheatsheet/README-zh.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/community/blob/master/contributors/guide/contributor-cheatsheet/README-zh.md&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;创建google账号&#34;&gt;创建google账号&lt;/h2&gt;
&lt;p&gt;推荐申请一个google邮箱。&lt;/p&gt;
&lt;p&gt;在开发机配置git&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git config --global user.email name@gmail.com   
$ git config user.email name@gmail.com            
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;签cla并阅读coc&#34;&gt;签CLA并阅读CoC&lt;/h2&gt;
&lt;p&gt;个人开发者需要签署CLA, 选择individual contributors并用github账号登陆，然后去邮箱授权；之后重新进入，再去邮箱签字。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://identity.linuxfoundation.org/projects/cncf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://identity.linuxfoundation.org/projects/cncf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;读一下CoC和CV&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/cncf/foundation/blob/master/code-of-conduct-languages/zh.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/cncf/foundation/blob/master/code-of-conduct-languages/zh.md&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/community/blob/master/values.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/community/blob/master/values.md&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;配置开发环境&#34;&gt;配置开发环境&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/README.md#setting-up-your-dev-environment-coding-and-debugging&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/community/blob/master/contributors/devel/README.md#setting-up-your-dev-environment-coding-and-debugging&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;可以在docker里面编译，也可以直接在操作系统上编译。&lt;/p&gt;
&lt;p&gt;在docker里面编译&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/kubernetes/blob/ae9ca48f01ddb03731e7903cfe91ef3db9ce8990/build/README.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/kubernetes/blob/ae9ca48f01ddb03731e7903cfe91ef3db9ce8990/build/README.md&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;build/run.sh make 只编译linux平台
build/run.sh make cross 编译所有平台
build/run.sh make kubectl KUBE_BUILD_PLATFORMS=darwin/amd64 编译指定平台的指定组件

build/run.sh make test 单元测试
build/run.sh make test-integration 集成测试
build/run.sh make test-cmd  命令行测试

build/copy-output.sh 将编译的binary从_output/dockerized/bin拷贝到本地

build/make-clean.sh 清空_output

build.shell.sh 交互模式进入编译的container

build/release.sh 编译，测试，打包，kubernetes.tar.gz
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在操作系统上编译&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/development.md#building-kubernetes-on-a-local-osshell-environment&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/community/blob/master/contributors/devel/development.md#building-kubernetes-on-a-local-osshell-environment&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;安装依赖&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get update
sudo apt-get install build-essential
// apt: docker, rsync, jq, go, 
// python: pyyaml,

// 安装etcd用于集成测试
./hack/install-etcd.sh
export PATH=&amp;quot;$GOPATH/src/k8s.io/kubernetes/third_party/etcd:${PATH}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;编译和测试&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make all  全部编译
make WHAT=cmd/kubectl  指定编译组件
make cross 编译所有平台
make cross KUBE_BUILD_PLATFORMS=windows/amd64 编译指定平台

make verify  presubmission verification测试
make test  单元测试
make test-integration  集成测试依赖etcd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/test-infra&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/test-infra&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;了解社区&#34;&gt;了解社区&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/community&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/community&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;因为项目组件很多，所以通过特别兴趣小组来分类。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SIG: Special Interest Groups&lt;/li&gt;
&lt;li&gt;WG: Working Group&lt;/li&gt;
&lt;li&gt;UG: User Group&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;KEP: Kubernetes Enhancement Proposal&lt;/p&gt;
&lt;p&gt;对于功能和API的修改需要在KEP提交议案讨论。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/enhancements&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/enhancements&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://slack.k8s.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://slack.k8s.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://discuss.kubernetes.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://discuss.kubernetes.io/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;选取修改的cl&#34;&gt;选取修改的CL&lt;/h2&gt;
&lt;p&gt;从github选取一个Issue进行修改。&lt;/p&gt;
&lt;p&gt;推荐新手从“good first issue&amp;quot; 开始。&lt;/p&gt;
&lt;p&gt;也可以通过sig或者kind标签过滤。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen&amp;#43;is%3Aissue&amp;#43;label%3A%22good&amp;#43;first&amp;#43;issue%22&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;提交patch&#34;&gt;提交patch&lt;/h2&gt;
&lt;p&gt;github用户直接fork到自己账号，clone下来即可。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/kubernetes/kubernetes.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建开发分支并修改代码&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git checkout -b canux_dev
...
git add -A
git commit -m &amp;quot;...&amp;quot;
git push ...
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;code-review&#34;&gt;code review&lt;/h2&gt;
&lt;p&gt;提交之后trybots会自动构建，可以通过build dashboard查看各个平台build结果:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://build.golang.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://build.golang.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;查看自己的patch的review情况：&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://go-review.googlesource.com/dashboard/self&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://go-review.googlesource.com/dashboard/self&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;显示随时间变化的历史测试结果：&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://testgrid.k8s.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://testgrid.k8s.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;显示pow中的自动化测试情况：&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://prow.k8s.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://prow.k8s.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;查看所有PR:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://gubernator.k8s.io/pr/all&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://gubernator.k8s.io/pr/all&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;把相似的失败聚合在一起以便排除故障:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://storage.googleapis.com/k8s-triage/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://storage.googleapis.com/k8s-triage/index.html&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>K8S API</title>
        <link>https://canuxcheng.com/post/k8s_api/</link>
        <pubDate>Fri, 10 Jan 2020 20:55:52 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_api/</guid>
        <description>&lt;h1 id=&#34;api&#34;&gt;API&lt;/h1&gt;
&lt;p&gt;api-server统一的操作入口.&lt;/p&gt;
&lt;p&gt;kubectl, UI, 等都是通过api-server操作资源.&lt;/p&gt;
&lt;p&gt;payload可以是json，也可以是yaml.&lt;/p&gt;
&lt;p&gt;yaml文件中#表示行注释。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;yaml&#34;&gt;yaml&lt;/h1&gt;
&lt;p&gt;部署k8s可以通过yaml文件来配置资源.&lt;/p&gt;
&lt;p&gt;资源对象组成部分:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: 
kind: 
metadata: 元数据
spec: 期望的状态
status: 观测到的状态
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看apiVersion:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl api-versions
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看Kind:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl api-resources

# In a namespace
kubectl api-resources --namespaced=true

# Not in a namespace
kubectl api-resources --namespaced=false
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;metadata:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;metadata:

  name:
  namespace:

  labels/标签: 用户筛选资源，唯一的资源组合方法, 可以使用selector来查询.

  annotations/注解: 存储资源的非标识性信息，扩展资源的spec/status.

  ownerReference/关系: 方便反向查找创建资源的对象，方便进行级联删除。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;spec:&lt;/p&gt;
&lt;p&gt;status:&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;调度抢占驱逐&#34;&gt;调度，抢占，驱逐&lt;/h1&gt;
&lt;p&gt;taints: 污点，使节点排斥特定pod。应用于node。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;taints:
- effect: NoSchedule
  key: kubernetes.io/arch
  value: arm64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;tolerations: 容忍度，使pod被吸引到特定节点。应用于pod。
这个只能让pod能部署到加了污点的node，pod也能部署到其它没有加污点的node。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tolerations:
- key: &amp;quot;key1&amp;quot;
  operator: &amp;quot;Equal&amp;quot;/&amp;quot;Exists&amp;quot;
  value: &amp;quot;value1&amp;quot;
  effect: &amp;quot;NoSchedule&amp;quot;/&amp;quot;NoExecute&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;affinity: 亲和力，affinity可以通过label指定pod部署到node。
但是不能保证其它pod不部署到这个node。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/arch
            operator: In
            values:
            - arm64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;nodeSelector: 节点选择，&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; nodeSelector:
   kubernetes.io/arch: arm64
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;pod&#34;&gt;Pod&lt;/h1&gt;
&lt;p&gt;pod模板， 通常使用deployment, job和statefulset, daemonset来管理pod.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: test
  namespace: test
  labels:
      app: test
spec:

  //// containers
  os:  // 操作系统模板
    name:

  imagePullSecrets:  // 私有镜像授权
  - name: my-harbor

  initContainers: //  initcontainer模板
  - name: init
    image: my-image
    command: ...
    args: ...

  containers: // container 模板
  - name: test
    // image
    image: image
    imagePullPolicy: Always/IfNotPresent/Never

    // entrypoint
    command:
    args:
    workingDir:

    // port
    ports:

    // resources
    resources:
      requests:  // 申明需要的资源
        memory: &amp;quot;64Mi&amp;quot;  // byte
        cpu: &amp;quot;250m&amp;quot;     // millicore (1 core = 1000 millicore)
        ephemeral-storage: &amp;quot;2Gi&amp;quot; // byte
      limits:
        memory: &amp;quot;128Mi&amp;quot;
        cpu: &amp;quot;500m&amp;quot;
        ephemeral-storage: &amp;quot;4Gi&amp;quot;

    // environment variables, 针对单个键值对.
    env:
    - name: key
      value: value
    - name: key
      valueFrom: // 将cm-name中的值cm-key传给key
        configMapKeyRef:
          name: cm-name
          key: cm-key
          optional:
    - name: key
      valueFrom: // 挂载secret
        secretKeyRef:
          key:
          name:
          optional:
        fieldRef:
        resourceFieldRef:

    // 环境变量，针对文件中所有键值对.
    envFrom:
    - configMapRef    // 将my-cm中的所有键值对变成环境变量.
        name: my-cm
        optional:
    - secretRef
        name: 
        optional:
     
    // volumeMounts (去Volume找对应资源)
    // 如果没有subpath，整个目录会被覆盖，目录下只有secret/configmap挂载的文件.
    volumeMounts: // secret以文件形式挂载到/etc/foo
    - name: my-secret
      mountPath: &amp;quot;/etc/foo&amp;quot; // 挂载之后覆盖整个目录
      readOnly: true
    - name: my-configmap
      mountPath: &amp;quot;/etc/bar&amp;quot; // 挂载之后覆盖整个目录
      // 如果有subpath, secret/configmap里的data里的文件名需要与subpath和mountpath指定的文件名一致.
    - name: config
      mountPath: /etc/app/app.conf  // 是文件，文件名要和subpath一致。
      subPath: app.conf // 挂载之后只覆盖目录中同名文件,其它文件不影响.

    // lifecycle
    livenessProbe:
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 5
      httpGet:
        path: /admin
        port: django
        # httpHeaders:
        # - name: Authorization
          # value: Basic $LDAP_ACCOUNT
    readinessProbe:
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 5
      httpGet:
        path: /admin
        port: django
        # httpHeaders:
        # - name: Authorization
          # value: Basic $LDAP_ACCOUNT

    // securityContext
    securityContext:

    // debugging
    stdin: false
    stdinOnce: false
    tty: false

  //// security context
  securityContext: // pod级别security context定义
    runAsuser: 1000
    runAsGroup: 3000
    fsGroup: 2000

  //// volumes
  volumes:
  - name: my-secret // 指定要挂载的secret
    secret:
      secretName: mysecret
  - name: my-configmap
    configMap:
      name: myconfigmap

  //// lifecycle
  restartPolicy:

  //// scheduling
  nodeName:
  nodeSelector: // 将pod部署到指定node
    key: value
  affinity:
  tolerations:

  //// others
  hostname:
  hostNetwork:
  serviceAccountName:
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;pod中的container共享存储(pod volume):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
Kind: Pod
medadata:
spec:

  # 两种pod volume
  volumes:
  # emptyDir： pod删除之后该目录也会被删除
  - name: cache-volume
    emptyDir: {}
  # hostPath: pod删除之后该目录还在host上. 
  - name: hostpath-volume
    hostPath:
      path: /path/on/host

  containers:
  - name: container1
    image: test
    volumeMounts:
    - name: cache-volume
      mountpath: /path/on/container
      # subPath会在emptyDir或hostPath目录下创建子目录
      subPath: cache1
  - name: container2
    image: test
    volumeMounts:
    - name: hostpath-volume
      mountpath: /path/on/container
      readOnly: true
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;deployment&#34;&gt;Deployment&lt;/h1&gt;
&lt;p&gt;用于部署无状态服务。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-deploy
  namespace: my-ns
  lables:
    app: my-app
spec:
  replicas: 3
  # 选择器
  selector: 
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app
        image: image:latest
        imagePullPolicy: IfNotPresent/Always
        ports:
        - containerPort: 443
        volumeMounts:
        - name: my-hostpath
          mountPath: /path/on/cpod
        - name: my-pvc
          mountPath: /data 
      volumes:
      - name: my-hostpath
        hostPath: 
          path: /path/on/host
      - name: my-pvc
        persistentVolumeClaim:
          claimName: nfs-pvc
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;daemonset&#34;&gt;DaemonSet&lt;/h1&gt;
&lt;p&gt;每个node上部署一个pod，用于部署agent。&lt;/p&gt;
&lt;p&gt;DaemonSet&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-ds
  namespace: my-ns
  labels:
    k8s-app: my-app
  spec:
    selector:
      matchLabels:
        name: my-app
    template:
      metadata:
        labels:
          name: my-app
      spec:
        containers:
        - name: my-container
          image: my-img
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;statefulset&#34;&gt;StatefulSet&lt;/h1&gt;
&lt;p&gt;用于部署有状态服务。&lt;/p&gt;
&lt;p&gt;StatefulSet 中的 Pod 拥有一个唯一的顺序索引和稳定的网络身份标识。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
  namespace: test
  labels:
    k8s-app: my-app
spec:
  serviceName: &amp;quot;nginx&amp;quot;
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      resources:
        requests:
          storage: 1Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;job&#34;&gt;Job&lt;/h1&gt;
&lt;p&gt;Job&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;appVersion: batch/v1
kind: Job
metadata:
  name: my-job
spec:
  # 代表本pod队列执行此次数(被执行8次)
  completions: 8
  # 代表并行执行个数(同时有两个在运行)
  parallelism: 2
  backoffLimit: 4
  template:
    spec:
      containers:
      - name: my-job
        image: my-image
        conmand: [&#39;test&#39;]
      restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;cronjob&#34;&gt;CronJob&lt;/h1&gt;
&lt;p&gt;CronJob&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: batch/v1
kind: CronJob
metadata:
  name: my-cj
spec:
  schedule: &amp;quot;* * * * *&amp;quot;
  startingDeadlineSeconds: None(default)/10
  concurrencyPolicy: Allow(default)/Forbid/Replace
  suspend: false(default)/true
  successfulJobsHistoryLimit: 3(default)
  failedJobsHistoryLimit: 1(default)
  jobTemplate:
    spec:
      template:
        metadata:
          annotations: ...
          labels: ...
        spec:
          nodeSelector:
            ...
          imagePullSecrets:
            ...
          restartPolicy: OnFailure
          containers:
          - name: image
            image: image
            args:
            - /bin/sh
            - -c
            - date
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;configmap&#34;&gt;ConfigMap&lt;/h1&gt;
&lt;p&gt;configmap只能在当前namespace使用.&lt;/p&gt;
&lt;p&gt;configmap的配置在pod中无法修改绑定的文件.&lt;/p&gt;
&lt;p&gt;data里面的文件名就是挂载之后的文件名。&lt;/p&gt;
&lt;p&gt;ConfigMap&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: flanel
    tier: node
  name: flannel-cfg
  namespace: kube-system
data:
  cni-conf.json: |
    {
      &amp;quot;name&amp;quot;: &amp;quot;n1&amp;quot;
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建配置文件的configmap&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl -n app create cm my-conf --from-file ./config.ini -o yaml &amp;gt; myconf-configmap.yaml
$ kubectl -n influxdata create cm dashboard-docker --from-file Docker.json -o yaml &amp;gt; grafana-dashboard-docker-configmap.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;secret&#34;&gt;Secret&lt;/h1&gt;
&lt;p&gt;secret只能在当前namespace使用.&lt;/p&gt;
&lt;p&gt;data里的文件名就是挂载之后的文件名。&lt;/p&gt;
&lt;p&gt;Opaque是用户自定义格式&lt;/p&gt;
&lt;p&gt;generic secret&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create secret generic empty-secret

apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: mysecret
  namespace: kube-system
data:
  username: name 
  password: pw
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建tls secret账号:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl -n kubernetes-dashboard create secret tls \
kubernetes-dashboard-tls --key ca.key --cert ca.crt 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;类型为 kubernetes.io/tls 的 Secret 中包含密钥和证书的 DER 数据，以 Base64 格式编码。 如果你熟悉私钥和证书的 PEM 格式，base64 与该格式相同，只是你需要略过 PEM 数据中所包含的第一行和最后一行。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Secret
metadata:
  name: secret-tls
type: kubernetes.io/tls
data:
  tls.crt: |
    MIIC2DCCAcCgAwIBAgIBATANBgkqh ...    
  tls.key: |
    MIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ ...   
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建docker secret:&lt;/p&gt;
&lt;p&gt;kubernetes.io/dockercfg	~/.dockercfg 文件的序列化形式&lt;/p&gt;
&lt;p&gt;kubernetes.io/dockerconfigjson	~/.docker/config.json 文件的序列化形式&lt;/p&gt;
&lt;p&gt;给一个private registry创建secret:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl -n ns create secret docker-registry &amp;lt;name&amp;gt; \
--docker-server=https://harbor.domain.com --docker-username=user --docker-password=pw --docker-email=canuxcheng@gmail.com 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;根据本地的文件创建secret（如果需要多个registry，可以先在本地登陆）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl -n ns create secret generic regcred \
--from-file=.dockerconfigjson=$HOME/.docker/config.json \
--type=kubernetes.io/dockerconfigjson

apiVersion: v1
kind: Secret
metadata:
  name: artifactory-cred
  namespace: ...
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: ewoJImF1d......
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;service&#34;&gt;Service&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: grafana-service
  namespace: influxdata
spec:
  type: NodePort
  ports:
  - name: https
    port: 3000 // 集群内部访问的port.
    targetPort: 3000 // pod指定的port.
    nodePort: 32000 // 集群外部访问内部service的port.
  selector:   // 匹配资源的metadata.labels
    app: grafana
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;externalname-service&#34;&gt;ExternalName Service&lt;/h2&gt;
&lt;p&gt;ExternalName Service 是 Service 的特例，它没有选择算符，但是使用 DNS 名称, 将服务映射到 DNS 名称，而不是selector.&lt;/p&gt;
&lt;p&gt;访问其它namespace的service.&lt;/p&gt;
&lt;p&gt;当查找主机 my-service.my-ns.svc.cluster.local 时，集群 DNS 服务返回 CNAME 记录， 其值为 out-service.out-ns.svc.cluster.local。
访问 my-service 的方式与其他服务的方式相同，但主要区别在于重定向发生在 DNS 级别，而不是通过代理或转发&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: my-ns
spec:
  type: ExternalName
  externalName: out-service.out-ns.svc.cluster.local // 指向其它namespace的service.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;endpoint&#34;&gt;Endpoint&lt;/h2&gt;
&lt;p&gt;下面场景可以使用Endpoint.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;希望在生产环境中使用外部的数据库集群，但测试环境使用自己的数据库。&lt;/li&gt;
&lt;li&gt;希望服务指向另一个 命名空间 中或其它集群中的服务。&lt;/li&gt;
&lt;li&gt;您正在将工作负载迁移到 Kubernetes。 在评估该方法时，您仅在 Kubernetes 中运行一部分后端。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;先创建service:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: mysql-service
  namespace: influxdata
spec:
  ports:
    - protocol: TCP
      port: 3306
      targetPort: 3306
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;再创建endpoint：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Endpoints
metadata:
  name: mysql-service
  namespace: influxdata
subsets:
  - addresses:
      - ip: 10.103.X.X // 指向外部服务的IP
    ports:
      - port: 3306
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;hpc&#34;&gt;HPC&lt;/h1&gt;
&lt;p&gt;Horizontal Pod Autoscaler&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: 
  labels: 
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: d-name
  minReplicas: 2
  maxReplicas: 20
  metrics:
    - type: Resource
      resource:
        name: cpu
        targetAverageUtilization: 80
    - type: Resource
      resource:
        name: memory
        targetAverageUtilization: 80
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;authentication&#34;&gt;Authentication&lt;/h1&gt;
&lt;p&gt;默认的ClusterRole和ClusterRoleBinding大部分是system:开头。&lt;/p&gt;
&lt;h2 id=&#34;serviceaccont&#34;&gt;ServiceAccont&lt;/h2&gt;
&lt;p&gt;服务账户是在具体名字空间的。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ServiceAccount
metadata:
  name: default
  namespace: default
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1.22之前k8s会自动给SA创建token.&lt;/p&gt;
&lt;p&gt;1.24之后使用TokenRequest获取有时间限制的token。&lt;/p&gt;
&lt;p&gt;创建持久化token&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Secret
type: kubernetes.io/service-account-token
metadata:
  name: mysecretname
  annotations:
    kubernetes.io/service-account.name: myserviceaccount
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;role&#34;&gt;Role&lt;/h2&gt;
&lt;p&gt;通过role来给指定ns内的资源授权.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [&amp;quot;&amp;quot;] # &amp;quot;&amp;quot; 标明 core API 组
  resources: [&amp;quot;pods&amp;quot;]
  verbs: [&amp;quot;get&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;list&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;rolebinding&#34;&gt;RoleBinding&lt;/h2&gt;
&lt;p&gt;将role或clusterrole权限赋予具体的角色.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1
# 此角色绑定允许 &amp;quot;jane&amp;quot; 读取 &amp;quot;default&amp;quot; 名字空间中的 Pods
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
# 你可以指定不止一个“subject（主体）”
- kind: User
  name: jane # &amp;quot;name&amp;quot; 是区分大小写的
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # &amp;quot;roleRef&amp;quot; 指定与某 Role 或 ClusterRole 的绑定关系
  kind: Role # 此字段必须是 Role 或 ClusterRole
  name: pod-reader     # 此字段必须与你要绑定的 Role 或 ClusterRole 的名称匹配
  apiGroup: rbac.authorization.k8s.io
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;clusterrole&#34;&gt;ClusterRole&lt;/h2&gt;
&lt;p&gt;clusterrole给整个集群授权.不需要namespace.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # &amp;quot;namespace&amp;quot; 被忽略，因为 ClusterRoles 不受名字空间限制
  name: secret-reader
rules:
- apiGroups: [&amp;quot;&amp;quot;]
  # 在 HTTP 层面，用来访问 Secret 对象的资源的名称为 &amp;quot;secrets&amp;quot;
  resources: [&amp;quot;secrets&amp;quot;]
  verbs: [&amp;quot;get&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;list&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;clusterrolebinding&#34;&gt;ClusterRoleBinding&lt;/h2&gt;
&lt;p&gt;跨集群授权（也就是要访问不同ns的资源).不需要namespace.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1
# 此集群角色绑定允许 “manager” 组中的任何人访问任何名字空间中的 secrets
kind: ClusterRoleBinding
metadata:
  name: read-secrets-global
subjects:
- kind: Group
  name: manager # &#39;name&#39; 是区分大小写的
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Operator</title>
        <link>https://canuxcheng.com/post/k8s_operator/</link>
        <pubDate>Fri, 10 Jan 2020 20:55:52 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_operator/</guid>
        <description>&lt;h1 id=&#34;operator&#34;&gt;Operator&lt;/h1&gt;
&lt;p&gt;TPR(Third Party Resource) 在k8s 1.7 被集成，并命名为CRD(Custom Resource Definition).&lt;/p&gt;
&lt;p&gt;通过CRD，K8S可以动态的添加和管理资源，controller跟踪这些资源。&lt;/p&gt;
&lt;p&gt;CRD+custom Controller = decalartive API(声明式API),一般分为通用性controller和operator.&lt;/p&gt;
&lt;p&gt;通用型controller一般用于平台需求，operator一般用于部署特定应用.&lt;/p&gt;
&lt;p&gt;用于开发operator的工具有kubebuilder和operator-sdk, 他们都是基于controller-runtime开发.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/operator-framework/awesome-operators&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/operator-framework/awesome-operators&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.2/html/operators/index&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.2/html/operators/index&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://operatorhub.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://operatorhub.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;开发示例:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/sample-controller&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/sample-controller&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;operator的build三种模式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;go&lt;/li&gt;
&lt;li&gt;ansible&lt;/li&gt;
&lt;li&gt;helm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;operator的run三种模式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在集群外部本地运行（开发测试).&lt;/li&gt;
&lt;li&gt;作为deployment在集群内部运行.&lt;/li&gt;
&lt;li&gt;通过OLM部署.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;operator-sdk&#34;&gt;operator-sdk&lt;/h1&gt;
&lt;p&gt;redhat的operator-sdk可以方便的开发opeartor.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/operator-framework/operator-sdk&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/operator-framework/operator-sdk&lt;/a&gt;
&lt;a class=&#34;link&#34; href=&#34;https://sdk.operatorframework.io/docs/installation/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://sdk.operatorframework.io/docs/installation/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;kubebuilder&#34;&gt;kubebuilder&lt;/h1&gt;
&lt;p&gt;sig维护的kubebuilder也能方便的开发operator.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes-sigs/kubebuilder&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes-sigs/kubebuilder&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Service Discovery</title>
        <link>https://canuxcheng.com/post/k8s_servicediscovery/</link>
        <pubDate>Fri, 10 Jan 2020 20:55:52 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_servicediscovery/</guid>
        <description>&lt;h1 id=&#34;coordination--service-discovery&#34;&gt;Coordination &amp;amp; Service Discovery&lt;/h1&gt;
&lt;p&gt;微服务的服务注册和服务发现.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;coredns&lt;/li&gt;
&lt;li&gt;etcd&lt;/li&gt;
&lt;li&gt;zookeeper&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;etcd&#34;&gt;Etcd&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/etcd-io/etcd&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/etcd-io/etcd&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;类似的有consul和zoomkeeper.&lt;/p&gt;
&lt;h3 id=&#34;etcdctl&#34;&gt;etcdctl&lt;/h3&gt;
&lt;p&gt;使用证书访问:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ etcdctl \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt  \
--key=/etc/kubernetes/pki/etcd/server.key \
--insecure-skip-tls-verify=true \
&amp;lt;command&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看所有key&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ etcdctl get / --prefix --keys-only
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;zookeeper&#34;&gt;zookeeper&lt;/h2&gt;
</description>
        </item>
        <item>
        <title>Compose</title>
        <link>https://canuxcheng.com/post/cncf_compose/</link>
        <pubDate>Sat, 04 Jan 2020 21:53:40 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/cncf_compose/</guid>
        <description>&lt;h1 id=&#34;docker-compose&#34;&gt;docker-compose&lt;/h1&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/docker/compose&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/docker/compose&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;通过一个yaml文件来管理容器中的服务，包括网络和存储。&lt;/p&gt;
&lt;p&gt;安装:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://docs.docker.com/compose/install/
$ sudo curl -L https://github.com/docker/compose/releases/download/1.21.2/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
$ sudo chmod +x /usr/local/bin/docker-compose
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;docker-compose命令&#34;&gt;docker-compose命令&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;docker-compose [-f &amp;lt;arg&amp;gt;...] [options] [COMMAND] [ARGS...]
-f/--file
-p/--project-name # 默认目录名
-H/--host

# 拉取compose文件中指定的镜像
$ docker-compose -f service.yml pull

# 根据docker-compose.yml把stack打包成一个Distributed Application Bundles文件.
$ docker-compose bundle -o &amp;lt;project name&amp;gt;.dab

$ docker-compose start [servoce...]
$ docker-compose stop [service...]
$ docker-compose restart [service...]
$ docker-compose up -d [service...]
$ docker-compose down -v

$ docker-compose logs -f
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;docker-composeyml&#34;&gt;docker-compose.yml&lt;/h1&gt;
&lt;p&gt;compose中的变量：&lt;/p&gt;
&lt;p&gt;&amp;ldquo;x-&amp;ldquo;开头的会被compose忽略，但是会被yaml解析.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;x-var-name: &amp;amp;default-label
  key1: val1
  key2: val2
  
services:
  mysql:
    &amp;lt;&amp;lt;: *default-label
    image: mysql
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;compose文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;version: &amp;quot;3.6&amp;quot;
services:
  mongo:
    image: mongo:latest
    hostname: hostname
    init
    dns
    dns_search
    expose
    extra_hosts

    healthcheck:
      test: curl -f https://host/ping || exit 1
      test: wget --no-verbose --tries=1 --spider https://host/ping || exit 1
      interval:
      timeout:
      retries:
      stat_period:

    stop_grrace_period
    isolation
    pid
    profiles
    stop_signal
    labels:
      com.example.key: value

    // start: 下列选项不能用于swarm stack部署.
    sysctls(19.03+)
    privileged: true
    depends_on:
      - service-name
    build
    cgroup_parent
    container_name
    devices:
      - /dev/vboxdrv:/dev/vboxdrv
    tmpfs
    external_links
    links
    network_mode: bridge/host/none
    restart: no(default)
    security_opt
    userns_mode
    ulimits
    cap_add
    cap_drop
    // end

    networks:
    - mynetwork
    // 只有自定义网络可以指定静态IP
    networks:
      mynetwork:
        ipv4_address: 172.19.0.100

    volumes: // short syntax
    - /container/dir // 自动创建volume
    - myvolume:/container/dir 
    - /host/dir:/container/dir
    volumes: // long syntax
    - type: valume/bind/tmpfs
      source: 
      target:
      read_only:
      bind:
      volume:
      tmpfs:
      consistency:
    volumes:
    - &amp;quot;/path/to/file:/path/to/file 挂载文件
    
    configs:
    - my-conf
    configs:
    - source: config-name
      target: file-name
     
    secrets:
    - my-sec
    secrets:
    - source: secret-name
      target: file-name in /run/secrets/
    
    logging:
      driver: syslog
      options:
        syslog-address: &amp;quot;tcp://192.168.0.42:123&amp;quot;

    ports:  // long syntax
    - target: 80
      published: 8080
      mode: host
      protocol: tcp/udp
    ports: // short syntax
      - 80  // host上的port没有指定就是一个随机的port
      - 80:80
      - 1234:1234/udp

    // 设置环境变量
    environment:
      RABBITMQ_DEFAULT_USER: sandbox
      RABBITMQ_DEFAULT_PASS: password
    environment:
      - RABBITMQ_DEFAULT_USER=sandbox
      - RABBITMQ_DEFAULT_PASS=password
      - TZ=UTC   // 设置时区
    &amp;lt;https://docs.docker.com/compose/compose-file/#env_file&amp;gt;
    # When you set the same environment variable in multiple files, 
    # here’s the priority used by Compose to choose which value to use:
    # 1. Compose file (environment)
    # 2. Shell environment variables(export key=value)
    # 3. Environment file (env_file)
    # 4. Dockerfile
    # 5. Variable is not defined
    env_file:

    entrypoint:

    command: [&amp;quot;./wait-for-it.sh&amp;quot;, &amp;quot;db:5432&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;python&amp;quot;, &amp;quot;app.py&amp;quot;]

    // 块结构，和上面的[]等效.
    command: 
    - &amp;quot;./wait-for-it.sh&amp;quot;
    - &amp;quot;:b:5432&amp;quot;
    - &amp;quot;--&amp;quot;
    - &amp;quot;python&amp;quot;
    - &amp;quot;app.py&amp;quot;

    // 解析为: &amp;quot;line1 line2\n&amp;quot;,  会自动加换行符.
    command: &amp;gt;
    line1
    line2

    // 解析为: &amp;quot;line1 line2&amp;quot;, 没有换行符.
    command: &amp;gt;-
    line1
    line2

    // start:  下面三个deploy下面的字段，也能用于非swarm模式.
    // 在compose v3 中针对非swarm模式的container做资源限制等操作.
    // --compatibility If set, Compose will attempt to convert keys in v3 files to their non-Swarm equivalent
    $ docker-compose --compatibility up -d
    // --compatibility 支持下面三种key:
    replicas:
    restart_policy:
      condition: any(default)
      max_attempts:
    resources:
      limits:
        cpus: &#39;0.5&#39;
        memory: 1G
      reservations:
        cpus: &#39;0.25&#39;
        memory: 20M


// start: 定义config/secret/volume/netework资源
configs:
  c-name:
    // 根据文件创建
    file: ./httpd.conf
    // 使用已经创建好的
    external: true

secrets:
  s-name:
    // 根据文件创建
    file: ./server.cert
    // 使用已经创建好的
    external: true

volumes:
  data-volume:

// 以定义好的volume
volumes:
  data:
    external: true

// (推荐)使用已经创建好的网络
// 通过命令行或者api 创建网络
networks:
  mynetwork:
    external: true
    name: lan0 // 通过docker network ls 查看名字

// 创建bridge网络
// 会在网络名字自动加namespace
networks:
  mynetwork:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: lan0
    ipam:
      driver: default
      config:
        - subnet: 192.168.1.0/24

// 创建overlay网络
// 会在网络名字自动加namespace
networks:
  ol0:
    driver: overlay
    attachable: true
    driver_opts:
      com.docker.network.bridge.name: ol0
    ipam:
      driver: default
      config:
      - subnet: 172.12.0.0/16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;compose文件中用到的变量&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.Service.ID Service ID 
.Service.Name Service name 
.Service.Labels Service labels 
.Node.ID Node ID 
.Node.Hostname Node Hostname 
.Task.ID Task ID 
.Task.Name Task name 
.Task.Slot Task slot 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;yaml变量：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;key: &amp;amp;varhash    value
key1: *varhash

# 定义变量
list: 
  &amp;amp;varlist
  key1: value1
  key2: value2
# 将list的元素赋值给list1
list1: 
  &amp;lt;&amp;lt;: *varlist   
  key3: value2
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;kompose&#34;&gt;Kompose&lt;/h1&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/kompose&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/kompose&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;install:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ curl -L https://github.com/kubernetes/kompose/releases/download/v1.19.0/kompose-linux-amd64 -o kompose

$ chmod +x kompose
$ sudo mv ./kompose /usr/local/bin/kompose
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>API Access Control</title>
        <link>https://canuxcheng.com/post/k8s_accesscontrol/</link>
        <pubDate>Sat, 04 Jan 2020 20:03:39 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_accesscontrol/</guid>
        <description>&lt;h1 id=&#34;api-access-control&#34;&gt;API Access Control&lt;/h1&gt;
&lt;h2 id=&#34;admission-controllers&#34;&gt;Admission Controllers&lt;/h2&gt;
&lt;h3 id=&#34;mutatingadmissionwebhook&#34;&gt;MutatingAdmissionWebhook&lt;/h3&gt;
&lt;h3 id=&#34;validatingadmissionwebhook&#34;&gt;ValidatingAdmissionWebhook&lt;/h3&gt;
&lt;h3 id=&#34;validatingwebhookconfiguration&#34;&gt;ValidatingWebhookConfiguration&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  annotations:
    meta.helm.sh/release-name: ingress-nginx-internal
    meta.helm.sh/release-namespace: ingress-nginx
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx-internal
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.9.1
    helm.sh/chart: ingress-nginx-4.8.1
  name: ingress-nginx-internal-admission
webhooks:
- admissionReviewVersions:
  - v1
  clientConfig:
    caBundle: 
    service:
      name: ingress-nginx-internal-controller-admission
      namespace: ingress-nginx
      path: /networking/v1/ingresses
      port: 443
  failurePolicy: Fail
  matchPolicy: Equivalent
  name: validate.nginx.ingress.kubernetes.io
  namespaceSelector: {}
  objectSelector: {}
  rules:
  - apiGroups:
    - networking.k8s.io
    apiVersions:
    - v1
    operations:
    - CREATE
    - UPDATE
    resources:
    - ingresses
    scope: &#39;*&#39;
  sideEffects: None
  timeoutSeconds: 10
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;mutatingwebhookconfiguration&#34;&gt;MutatingWebhookConfiguration&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  annotations:
    meta.helm.sh/release-name: vault-secrets-webhook
    meta.helm.sh/release-namespace: vault-secrets-webhook
  labels:
    app.kubernetes.io/managed-by: Helm
  name: vault-secrets-webhook
webhooks:
- admissionReviewVersions:
  - v1beta1
  clientConfig:
    caBundle:
    service:
      name: vault-secrets-webhook
      namespace: vault-secrets-webhook
      path: /pods
      port: 443
  failurePolicy: Ignore
  matchPolicy: Equivalent
  name: pods.vault-secrets-webhook.admission.banzaicloud.com
  namespaceSelector:
    matchExpressions:
    - key: name
      operator: NotIn
      values:
      - kube-system
    - key: kubernetes.io/metadata.name
      operator: NotIn
      values:
      - kube-system
    - key: kubernetes.io/metadata.name
      operator: NotIn
      values:
      - vault-secrets-webhook
  objectSelector:
    matchExpressions:
    - key: security.banzaicloud.io/mutate
      operator: NotIn
      values:
      - skip
  reinvocationPolicy: Never
  rules:
  - apiGroups:
    - &#39;*&#39;
    apiVersions:
    - &#39;*&#39;
    operations:
    - CREATE
    resources:
    - pods
    scope: &#39;*&#39;
  sideEffects: NoneOnDryRun
  timeoutSeconds: 10
- admissionReviewVersions:
  - v1beta1
  clientConfig:
    caBundle:
    service:
      name: vault-secrets-webhook
      namespace: vault-secrets-webhook
      path: /secrets
      port: 443
  failurePolicy: Ignore
  matchPolicy: Equivalent
  name: secrets.vault-secrets-webhook.admission.banzaicloud.com
  namespaceSelector:
    matchExpressions:
    - key: name
      operator: NotIn
      values:
      - kube-system
    - key: kubernetes.io/metadata.name
      operator: NotIn
      values:
      - kube-system
    - key: kubernetes.io/metadata.name
      operator: NotIn
      values:
      - vault-secrets-webhook
  objectSelector:
    matchExpressions:
    - key: owner
      operator: NotIn
      values:
      - helm
    - key: security.banzaicloud.io/mutate
      operator: NotIn
      values:
      - skip
  reinvocationPolicy: Never
  rules:
  - apiGroups:
    - &#39;*&#39;
    apiVersions:
    - &#39;*&#39;
    operations:
    - CREATE
    - UPDATE
    resources:
    - secrets
    scope: &#39;*&#39;
  sideEffects: NoneOnDryRun
  timeoutSeconds: 10
- admissionReviewVersions:
  - v1beta1
  clientConfig:
    caBundle:
    service:
      name: vault-secrets-webhook
      namespace: vault-secrets-webhook
      path: /configmaps
      port: 443
  failurePolicy: Ignore
  matchPolicy: Equivalent
  name: configmaps.vault-secrets-webhook.admission.banzaicloud.com
  namespaceSelector:
    matchExpressions:
    - key: name
      operator: NotIn
      values:
      - kube-system
    - key: kubernetes.io/metadata.name
      operator: NotIn
      values:
      - kube-system
    - key: kubernetes.io/metadata.name
      operator: NotIn
      values:
      - vault-secrets-webhook
  objectSelector:
    matchExpressions:
    - key: owner
      operator: NotIn
      values:
      - helm
    - key: security.banzaicloud.io/mutate
      operator: NotIn
      values:
      - skip
  reinvocationPolicy: Never
  rules:
  - apiGroups:
    - &#39;*&#39;
    apiVersions:
    - &#39;*&#39;
    operations:
    - CREATE
    - UPDATE
    resources:
    - configmaps
    scope: &#39;*&#39;
  sideEffects: NoneOnDryRun
  timeoutSeconds: 10
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Automation &amp; Configuration</title>
        <link>https://canuxcheng.com/post/cncf_platform/</link>
        <pubDate>Sat, 04 Jan 2020 20:03:39 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/cncf_platform/</guid>
        <description>&lt;h1 id=&#34;automation--configuration&#34;&gt;Automation &amp;amp; Configuration&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Cloud Custodian&lt;/li&gt;
&lt;li&gt;kubeedge&lt;/li&gt;
&lt;li&gt;pulumi&lt;/li&gt;
&lt;li&gt;terraform&lt;/li&gt;
&lt;li&gt;opentofu&lt;/li&gt;
&lt;li&gt;kratix&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;platform orchestration.&lt;/p&gt;
&lt;h2 id=&#34;kratix&#34;&gt;kratix&lt;/h2&gt;
</description>
        </item>
        <item>
        <title>Dashboard</title>
        <link>https://canuxcheng.com/post/k8s_dashboard/</link>
        <pubDate>Sat, 04 Jan 2020 20:03:39 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_dashboard/</guid>
        <description>&lt;h1 id=&#34;dashboard-add-ons&#34;&gt;dashboard add-ons&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;kubernetes-dashboard&lt;/li&gt;
&lt;li&gt;lens&lt;/li&gt;
&lt;li&gt;octant&lt;/li&gt;
&lt;li&gt;weave scope&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;kubernetes-dashboard&#34;&gt;kubernetes-dashboard&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/dashboard&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/dashboard&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 部署dashboard
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml

// check
$ kubectl -n kubernetes-dashboard get pods --watch
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 删除已安装的dashboard
$ kubectl delete ns kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dashboard-arguments&#34;&gt;dashboard arguments&lt;/h3&gt;
&lt;p&gt;使用basic auth:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--enable-skip-login
--enable-insecure-login
--system-banner=&amp;quot;Welcome to Kubernetes&amp;quot;
--authentication-mode=&amp;quot;basic&amp;quot; // 默认是 token 登陆.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;access-control&#34;&gt;access control&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;kubeconfig&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;authorization header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;token&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;basic&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;username/password login&lt;/p&gt;
&lt;h3 id=&#34;access-dashboard&#34;&gt;access dashboard&lt;/h3&gt;
&lt;p&gt;本机访问&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl proxy
#&amp;gt; http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;远程访问&lt;/p&gt;
&lt;p&gt;port-forward:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 8080:443 --address 0.0.0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;nodePort:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  type: NodePort
  ports:
    - port: 443
    targetPort: 8443
    nodePort: 30001
selector:
  k8s-app: kubernetes-dashboard

#&amp;gt; https://&amp;lt;node-ip&amp;gt;:30001
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ingress:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; https://&amp;lt;ingress-host&amp;gt;:&amp;lt;ingress-port&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;metrics-server&#34;&gt;metrics-server&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes-sigs/metrics-server&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes-sigs/metrics-server&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;参数:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--kubelet-preferred-address-types
--kubelet-insecure-tls
--requestheader-client-ca-file
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;部署:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# deploy 0.3.6
# 修改image为  registry.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6
$ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Key Management</title>
        <link>https://canuxcheng.com/post/k8s_km/</link>
        <pubDate>Sat, 04 Jan 2020 20:03:39 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_km/</guid>
        <description>&lt;h1 id=&#34;key-management&#34;&gt;Key Management&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;spiffe&lt;/li&gt;
&lt;li&gt;spire&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Microservivce</title>
        <link>https://canuxcheng.com/post/k8s_microservice/</link>
        <pubDate>Sat, 04 Jan 2020 20:03:39 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_microservice/</guid>
        <description>&lt;h1 id=&#34;microservivce&#34;&gt;Microservivce&lt;/h1&gt;
&lt;p&gt;微服务是一种架构。&lt;/p&gt;
&lt;p&gt;常见的架构:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monolithic application 单体应用.&lt;/li&gt;
&lt;li&gt;SOA(service-oriented architecture) 面向服务的体系结构.&lt;/li&gt;
&lt;li&gt;MicroServices 微服务.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;微服务架构的服务治理包括:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;service registry 服务注册&lt;/li&gt;
&lt;li&gt;service discovery 服务发现&lt;/li&gt;
&lt;li&gt;observability 可观测性(metrics,logging,trace)&lt;/li&gt;
&lt;li&gt;流量管理&lt;/li&gt;
&lt;li&gt;安全&lt;/li&gt;
&lt;li&gt;控制&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;微服务应用可以通过容器化(docker, k8s)部署，也可以通过serverless方式部署.&lt;/p&gt;
&lt;p&gt;不同的语言有不同的微服务框架.
java的dubbo, sprint boot.
golang的go-kit,  go-zero, kratos.
python的zappa, nameko.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dapr&#34;&gt;Dapr&lt;/h2&gt;
&lt;p&gt;Dapr is a portable, event-driven, runtime for building distributed applications across cloud and edge.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Scheduling &amp; Orchestration</title>
        <link>https://canuxcheng.com/post/cncf_orchestration/</link>
        <pubDate>Sat, 04 Jan 2020 20:03:39 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/cncf_orchestration/</guid>
        <description>&lt;h1 id=&#34;scheduling--orchestration&#34;&gt;Scheduling &amp;amp; Orchestration&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;docker swarm.&lt;/li&gt;
&lt;li&gt;KEDA&lt;/li&gt;
&lt;li&gt;Crossplane&lt;/li&gt;
&lt;li&gt;Knative&lt;/li&gt;
&lt;li&gt;Kubeflow&lt;/li&gt;
&lt;li&gt;Volcano&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;crossplane&#34;&gt;Crossplane&lt;/h2&gt;
</description>
        </item>
        <item>
        <title>Serverless</title>
        <link>https://canuxcheng.com/post/k8s_serverless/</link>
        <pubDate>Sat, 04 Jan 2020 20:03:39 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_serverless/</guid>
        <description>&lt;h1 id=&#34;serverless&#34;&gt;Serverless&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;knative&#34;&gt;Knative&lt;/h2&gt;
&lt;p&gt;Knative is a developer-focused serverless application layer which is a great complement to the existing Kubernetes application constructs. Knative consists of three components: an HTTP-triggered autoscaling container runtime called “Knative Serving”, a CloudEvents-over-HTTP asynchronous routing layer called “Knative Eventing”, and a developer-focused function framework which leverages the Serving and Eventing components, called &amp;ldquo;Knative Functions&amp;rdquo;.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Service Mesh</title>
        <link>https://canuxcheng.com/post/k8s_servicemesh/</link>
        <pubDate>Sat, 04 Jan 2020 20:03:39 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_servicemesh/</guid>
        <description>&lt;h1 id=&#34;service-mesh&#34;&gt;Service Mesh&lt;/h1&gt;
&lt;p&gt;servivce mesh是cncf基于sidecar推出的下一代面向云原生的微服务架构，是微服务基础设施, 用于处理微服务通信、治理、控制、可观测、安全等问题，具备业务无侵入、多语言、热升级等诸多特性.&lt;/p&gt;
&lt;p&gt;sidecar: 边车模式，就是把业务无关的功能，日志记录、监控、流量控制、服务注册、服务发现、服务限流、服务熔断、鉴权、访问控制和服务调用可视化等独立出来。&lt;/p&gt;
&lt;p&gt;特点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用程序通信的中间层&lt;/li&gt;
&lt;li&gt;轻量级网络代理&lt;/li&gt;
&lt;li&gt;应用程序无感知&lt;/li&gt;
&lt;li&gt;解耦应用程序的重试、超时、监控、追踪和服务发现.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Service Mesh是建立在物理或者虚拟网络层之上的，基于策略的微服务的流量控制，与一般的网络协议不同的是它有以下几个特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;开发者驱动&lt;/li&gt;
&lt;li&gt;可配置策略&lt;/li&gt;
&lt;li&gt;服务优先的网络配置而不是协议&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;istio&#34;&gt;Istio&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;linkerd&#34;&gt;Linkerd&lt;/h2&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>Service Proxy</title>
        <link>https://canuxcheng.com/post/k8s_proxy/</link>
        <pubDate>Sat, 04 Jan 2020 20:03:39 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_proxy/</guid>
        <description>&lt;h1 id=&#34;service-proxy&#34;&gt;Service Proxy&lt;/h1&gt;
&lt;p&gt;ingress =&amp;gt; gateway api&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;envoy&lt;/li&gt;
&lt;li&gt;contour&lt;/li&gt;
&lt;li&gt;traefik proxy&lt;/li&gt;
&lt;li&gt;haproxy&lt;/li&gt;
&lt;li&gt;metaLB&lt;/li&gt;
&lt;li&gt;nginx&lt;/li&gt;
&lt;li&gt;openelb&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;ingress-controller&#34;&gt;ingress controller&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ingress-nginx(nginx)&lt;/li&gt;
&lt;li&gt;aws-load-balancer-controller(alb)&lt;/li&gt;
&lt;li&gt;ingress-gce&lt;/li&gt;
&lt;li&gt;Traefik&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The kubernetes.io/ingress.class annotation is deprecated from kubernetes v1.22+.通过IngressClasses来选择ingress controller。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ingressClassName: nginx
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ingress 语法&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minimal-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx-example
  defaultBackend:
    resource:
      apiGroup: k8s.example.com
      kind: StorageBucket
      name: static-assets
  rules:
  - http:
      paths:
      - path: /testpath
        pathType: Prefix
        backend:
          service:
            name: test
            port:
              number: 80
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ImplementationSpecific：对于这种路径类型，匹配方法取决于 IngressClass。 具体实现可以将其作为单独的 pathType 处理或者与 Prefix 或 Exact 类型作相同处理。&lt;/p&gt;
&lt;p&gt;Exact：精确匹配 URL 路径，且区分大小写。&lt;/p&gt;
&lt;p&gt;Prefix：基于以 / 分隔的 URL 路径前缀匹配。匹配区分大小写，并且对路径中的元素逐个完成。 路径元素指的是由 / 分隔符分隔的路径中的标签列表。 如果每个 p 都是请求路径 p 的元素前缀，则请求与路径 p 匹配。&lt;/p&gt;
&lt;p&gt;ingressclass没有namespace。&lt;/p&gt;
&lt;h2 id=&#34;gateway-api-controller&#34;&gt;gateway api controller&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cilium&lt;/li&gt;
&lt;li&gt;contour&lt;/li&gt;
&lt;li&gt;GKE&lt;/li&gt;
&lt;li&gt;EKS&lt;/li&gt;
&lt;li&gt;kong&lt;/li&gt;
&lt;li&gt;traefik&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;ingress-nginx&#34;&gt;ingress-nginx&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/ingress-nginx&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/ingress-nginx&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 部署
 $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.34.1/deploy/static/provider/baremetal/deploy.yaml

// 验证部署
$ kubectl get pods --all-namespaces -l app.kubernetes.io/name=ingress-nginx --watch

// Detect installed version
POD_NAMESPACE=ingress-nginx
POD_NAME=$(kubectl get pods -n $POD_NAMESPACE -l app.kubernetes.io/name=ingress-nginx -o jsonpath=&#39;{.items[0].metadata.name}&#39;)
$ kubectl exec -it $POD_NAME -n $POD_NAMESPACE -- /nginx-ingress-controller --version
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;tls:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/tls/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.github.io/ingress-nginx/user-guide/tls/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;traefik&#34;&gt;traefik&lt;/h2&gt;
&lt;p&gt;traefik2.2+&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/traefik/traefik&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/traefik/traefik&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;install with helm:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm repo add traefik https://helm.traefik.io/traefik
helm repo update
helm install --create-namespace traefik -n traefik traefik traefik/traefik -f ./value.yaml

# expose dashboard:
kubectl port-forward -n traefik $(kubectl get pods -n traefik --selector &amp;quot;app.kubernetes.io/name=traefik&amp;quot; --output=name) 9000:9000 --address 0.0.0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;请求模型&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Client =&amp;gt; Traefik =&amp;gt; Backend
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;端口:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;9000: traefik管理页面端口
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;重要组件&#34;&gt;重要组件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Providers: 自动发现平台上的服务.&lt;/li&gt;
&lt;li&gt;Entrypoints: 监听传入的流量，定义接受请求的端口.&lt;/li&gt;
&lt;li&gt;Routers: 分析请求，负责将传入请求连接到负责处理的服务上.&lt;/li&gt;
&lt;li&gt;Middlewares: 在routers转给services之前修改请求.&lt;/li&gt;
&lt;li&gt;Services/LB: 将请求转给应用, 负责配置处理请求的实际服务.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;配置&#34;&gt;配置&lt;/h3&gt;
&lt;p&gt;两种配置类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;静态配置: 启动时的配置，通过配置文件(/etc/traefik/traefik.[toml|yaml]，环境变量或命令行参数配置 providers和entrypoints等.&lt;/li&gt;
&lt;li&gt;动态配置: 动态的路由配置，定义系统如何处理请求,从providers获取动态配置.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;静态配置:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;entrypoints&lt;/li&gt;
&lt;li&gt;providers&lt;/li&gt;
&lt;li&gt;servertransport&lt;/li&gt;
&lt;li&gt;certificatesresolvers&lt;/li&gt;
&lt;li&gt;api&lt;/li&gt;
&lt;li&gt;ping&lt;/li&gt;
&lt;li&gt;experimental&lt;/li&gt;
&lt;li&gt;hostresolver&lt;/li&gt;
&lt;li&gt;accesslog&lt;/li&gt;
&lt;li&gt;log&lt;/li&gt;
&lt;li&gt;metrics(datadog, influxdb, prometheus,statsd)&lt;/li&gt;
&lt;li&gt;tracing(datadog, elastic, haystack, instana, jaeger, zipkin)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;全局配置:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--global.checknewversion
--global.sendanonymoususagge
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;控制Traefik到Backend的连接的参数serversTransport:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--serversTransport.insecureSkipVerify=true
# self-signed TLS CA.
--serversTransport.rootCAs=foo.crt,bar.crt
--serversTransport.maxIdleConnsPerHost=7
--serversTransport.forwardingTimeouts.dialTimeout=1s
--serversTransport.forwardingTimeouts.responseHeaderTimeout=1s
--serversTransport.forwardingTimeouts.idleConnTimeout=1s
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;kubernetes-provider&#34;&gt;kubernetes provider&lt;/h3&gt;
&lt;p&gt;kubernetes provider有三种类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ingress&lt;/li&gt;
&lt;li&gt;IngressRoute&lt;/li&gt;
&lt;li&gt;Gateway API&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;https--tls&#34;&gt;https &amp;amp; tls&lt;/h3&gt;
&lt;p&gt;traefik的证书可以是手动创建证书，也可以通过let&amp;rsquo;s encrypt自动创建&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: name
  namespace: ns
spec:
  tls:
    secretName: my-tls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通过Let&amp;rsquo;s encrypt来自动创建证书有三种验证方式（tls, http, dns).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- &amp;quot;--certificatesresolvers.myresolver.acme.httpchallenge=true&amp;quot;
- &amp;quot;--certificatesresolvers.myresolver.acme.httpchallenge.entrypoint=web&amp;quot;
- &amp;quot;--certificatesresolvers.myresolver.acme.tlschallenge=true&amp;quot;
- &amp;quot;--certificatesresolvers.myresolver.acme.tlschallenge.entrypoint=websecure&amp;quot;
- &amp;quot;--certificatesresolvers.myresolver.acme.email=canux.cheng@arm.com&amp;quot;
- &amp;quot;--certificatesresolvers.myresolver.acme.storage=acme.json&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>Kubeadm</title>
        <link>https://canuxcheng.com/post/k8s_kubeadm/</link>
        <pubDate>Mon, 30 Dec 2019 21:47:17 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_kubeadm/</guid>
        <description>&lt;h1 id=&#34;kubeadm&#34;&gt;kubeadm&lt;/h1&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/kubeadm&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/kubeadm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;kubeadm是k8s自带的部署集群的工具.&lt;/p&gt;
&lt;h1 id=&#34;install&#34;&gt;Install&lt;/h1&gt;
&lt;h2 id=&#34;准备工作&#34;&gt;准备工作&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;安装runtime&#34;&gt;安装runtime&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;默认的cgroup驱动时cgroupfs,如果系统是systemd，就会有两个cgroup driver，会出问题.&lt;/p&gt;
&lt;p&gt;如果修改cgroup driver需要同时修改CRI和kubelet.&lt;/p&gt;
&lt;p&gt;修改containerd的cgroup driver:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo vim /etc/containerd/config.toml
#disabled_plugins = [&amp;quot;cri&amp;quot;]
[plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.containerd.runtimes.runc.options]
  SystemdCgroup = true

$ sudo systemctl restart containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;修改kubelet的cgroup driver(kubeadm-config.yaml):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubeadm-config.yaml
kind: ClusterConfiguration
apiVersion: kubeadm.k8s.io/v1beta3
kubernetesVersion: v1.21.0    // kubelet --version
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cgroupDriver: systemd
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;安装kubeadm-kubelet-kubectl&#34;&gt;安装kubeadm, kubelet, kubectl&lt;/h2&gt;
&lt;p&gt;在每台机器上安装 kubeadm, kubelet, kubectl:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo apt-get update
$ sudo apt-get install -y apt-transport-https ca-certificates curl
$ sudo curl -fsSL https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - 
$ echo &amp;quot;deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main&amp;quot; | sudo tee /etc/apt/sources.list.d/kubernetes.list
$ sudo apt-get update
$ sudo apt-get --yes --allow-unauthenticated install kubeadm kubelet kubectl
$ sudo apt-mark hold kubelet kubeadm kubectl
$ sudo systemctl enable kubelet
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;kubeadm-cli&#34;&gt;Kubeadm CLI&lt;/h1&gt;
&lt;p&gt;init:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubeadm init 
--config kubeadm-config.yaml
--kubernetes-version &amp;lt;version&amp;gt; // kubelet --version
--apiserver-advertise-address &amp;lt;master&amp;gt; // 多网卡指定网卡IP
--image-repository &amp;lt;registry&amp;gt; // default: k8s.gcr.io
--pod-network-cidr &amp;lt;cidr&amp;gt; // 指定pod的cidr
--service-cidr &amp;lt;cidr&amp;gt; // default: 10.96.0.0/12
--service-dns-domain // default: cluster.local
--cri-socket // 如果安装了多个cri需要指定.
--ignore-preflight-errors
--upload-certs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;join:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubeadm join [apiserver-advertise-address] --token &amp;lt;token&amp;gt; --discovery-token-ca-cert-hash &amp;lt;hash&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;reset:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubeadm reset -f/--force
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;token:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubeadm token create/delete/generate/list
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;部署cluster&#34;&gt;部署Cluster&lt;/h1&gt;
&lt;h2 id=&#34;部署master&#34;&gt;部署master&lt;/h2&gt;
&lt;p&gt;关闭swap&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo swapoff -a
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;初始化&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo kubeadm init \
--pod-network-cidr=10.244.0.0/16 \
--apiserver-advertise-address=&amp;lt;IP&amp;gt; \
--kubernetes-version=v1.17.0 \
--image-repository=registry.aliyuncs.com/google_containers \
--cri-socket=/run/containerd/containerd.sock \
-v=6
// --config 一般使用默认即可.
// --pod-network-cidr=10.244.0.0/16 是固定用法，表示选择flannel为网络插件。
// --image-repository 指定registry, 默认是gcr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;配置当前帐号&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;部署网络插件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 在所有node上部署cni-plugin:
// &amp;lt;https://github.com/containernetworking/plugins/releases&amp;gt;
$ sudo mkdir -p /opt/cni/bin
// 下载并解压所有插件命令到该目录. 默认CNI_PATH=/opt/cni/bin

// &amp;lt;https://docs.cilium.io/en/stable/installation/k8s-install-kubeadm/&amp;gt;
// cilium会自动下载plugins到/opt/cni/bin.

// &amp;lt;https://github.com/flannel-io/flannel&amp;gt;
$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;配置master是否部署pod&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# enable master deploy pod (默认不部署pod到master)
kubectl taint nodes --all node-role.kubernetes.io/control-plane-

# disable master deploy pod
kubectl taint nodes &amp;lt;node&amp;gt; node-role.kubernetes.io/master=true:NoSchedule
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;部署node&#34;&gt;部署node&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;$ sudo swapoff -a

// 如果有vpn，kubeadm会自动下载安装
// 在所有node上部署cni-plugin:
// &amp;lt;https://github.com/containernetworking/plugins/releases&amp;gt;
$ sudo mkdir -p /opt/cni/bin
// 下载并解压所有插件命令到该目录.

$ sudo kubeadm join 192.168.1.1:6443 \
--token 8po0v5.m1qlbc7w0btq15of \
--discovery-token-ca-cert-hash sha256:21d8365e336d5218637ddf26e2ec5d91c7dd2de518dbe47973e089837b13265b
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;验证&#34;&gt;验证&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl get pods -n kube-system
$ kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;删除cluster&#34;&gt;删除cluster&lt;/h2&gt;
&lt;p&gt;所有node运行:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo kubeadm reset -f
// 自动停止kubelet并且删除下列文件和目录
[/etc/kubernetes/manifests /etc/kubernetes/pki]
[/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[/var/lib/etcd /var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;需要手动删除:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo rm -rf /etc/cni/net.d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有node上删除flannel的网络配置&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo ifconfig cni0 down
$ sudo ip link delete cni0
$ sudo ifconfig flannel.1 down
$ sudo ip link delete flannel.1
$ sudo rm -rf /run/flannel
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有node清空iptables&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo iptables -F
$ sudo iptables -X
$ sudo iptables -t nat -F
$ sudo iptables -t nat -X
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果使用了IPVS:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ipvsadm --clear
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;删除配置&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ rm -rf $HOME/.kube
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;部署ha-cluster&#34;&gt;部署HA Cluster&lt;/h1&gt;
&lt;p&gt;ha需要在所有master节点安装haproxy和keepalived.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/high-availability/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/high-availability/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#options-for-software-load-balancing&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#options-for-software-load-balancing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在master1上初始化：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo kubeadm init --config ./kubeadm.yaml -v=6 --upload-certs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;加入其它master:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo kubeadm join 192.168.1.200:8443 --token &amp;lt;token&amp;gt; --discovery-token-ca-cert-hash &amp;lt;hash&amp;gt; --control-plane --certificate-key &amp;lt;key&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;加入node:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo kubeadm join 192.168.1.200:8443 --token &amp;lt;token&amp;gt; --discovery-token-ca-cert-hash &amp;lt;hash&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;配置&#34;&gt;配置&lt;/h1&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/control-plane-flags/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/control-plane-flags/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用自定义配置:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo kubeadm init --config ./config.yaml -v=6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看默认配置:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubeadm config print init-defaults
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;配置kubeadm：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
localAPIEndpoint:
  advertiseAddress: 10.103.1.1 // master IP
  bindPort: 6443
nodeRegistration:
  criSocket: /run/containerd/containerd.sock
  name: debug // master hostname
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;配置kubernetes:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 定制control plane
&amp;lt;https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/control-plane-flags/&amp;gt;
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: 10.58.203.200:8443 // HA中haproxy的VIP和port
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
  podSubnet: 10.244.0.0/16 // for flannel
imageRepository: k8s.gcr.io
kubernetesVersion: v1.18.6
controllerManager:
  ...
  extraArgs:
    &amp;lt;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/&amp;gt;
    allocate-node-cidrs: &#39;true&#39;
    node-cidr-mask-size: &#39;16&#39; // flannel的SubNetLen
    cluster-cidr: &#39;10.0.0.0/8&#39; // flannel的Network
apiServer:
  timeoutForControlPlane: 4m0s
  &amp;lt;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/&amp;gt;
    extraArgs:
      advertise-address: 192.168.0.103
      ...
scheduler:
  ...
  &amp;lt;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/&amp;gt;
  extraArgs:
    ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;修改kubelet的cgroup driver:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cgroupDriver: systemd
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Minikube</title>
        <link>https://canuxcheng.com/post/k8s_minikube/</link>
        <pubDate>Mon, 30 Dec 2019 21:47:17 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_minikube/</guid>
        <description>&lt;h1 id=&#34;minikube&#34;&gt;minikube&lt;/h1&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.io/zh/docs/setup/learning-environment/minikube/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.io/zh/docs/setup/learning-environment/minikube/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;minikube 能快速创建k8s的开发集群，支持在虚拟机上创建，也支持裸机创建.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 在裸机上创建：
sudo minikube start --vm-driver=none
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Image</title>
        <link>https://canuxcheng.com/post/cncf_image/</link>
        <pubDate>Tue, 03 Dec 2019 21:50:47 +0800</pubDate>
        
        <guid>https://canuxcheng.com/post/cncf_image/</guid>
        <description>&lt;h1 id=&#34;image&#34;&gt;image&lt;/h1&gt;
&lt;p&gt;容器镜像&lt;/p&gt;
&lt;p&gt;docker image driver: aufs, btrfs, devicemapper, overlay.&lt;/p&gt;
&lt;h2 id=&#34;multi-platform-images&#34;&gt;multi-platform images&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.docker.com/build/building/multi-platform/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://docs.docker.com/build/building/multi-platform/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;qemu&#34;&gt;qemu&lt;/h3&gt;
&lt;p&gt;使用qume:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 安装bitfmt
docker run --privileged --rm tonistiigi/binfmt --install all

// 查看支持的平台
ls -l /proc/sys/fs/binfmt_misc/qemu-*
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;multiple-native-nodes&#34;&gt;multiple native nodes&lt;/h3&gt;
&lt;p&gt;安装&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// linux
sudo apt install docker-buildx-plugin

// mac
brew install docker-buildx
mkdir -p ~/.docker/cli-plugins
ln -sfn $(which docker-buildx) ~/.docker/cli-plugins/docker-buildx
docker buildx install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看版本&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker buildx version
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;管理builder instance&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker buildx create
--append 添加node到builder实例。
--leave 从builder实例删除node
--driver Driver to use(&amp;quot;docker&amp;quot;, &amp;quot;docker-container&amp;quot;, &amp;quot;kubernetes&amp;quot;)
--name
--use
--node
--platform 
--bootstrap 启动实例（以容器的形式启动）

// 以本地是amd64为例，创建一个实例.
docker buildx create --use --bootstrap --platform linux/amd64,linux/amd64/v2,linux/amd64/v3,linux/arm64,linux/arm/v7,linux/arm/v6 --name canux-builder

// 如果没有qumu，可以把不同平台的远程机器加到builder实例.
docker buildx create \
--name local_remote_builder \
--append --node &amp;lt;my-arm-server&amp;gt; \
--platform linux/arm64,linux/riscv64,linux/ppc64le,linux/s390x,linux/mips64le,linux/mips64,linux/arm/v7,linux/arm/v6 \
ssh://user@&amp;lt;my-arm-server&amp;gt; 

docker buildx rm

docker buildx stop

docker buildx inspect

docker buildx use

// 查看当前可用的builders
docker buildx ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;构建多平台镜像&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker buildx build --platform &amp;lt;p1,p2&amp;gt; ...

// 一次编译多个平台的镜像直接push到registry。
docker buildx build --platform linux/amd64,linux/amd64/v2,linux/amd64/v3,linux/arm64,linux/arm/v7,linux/arm/v6 --push -t name/target:tag .

// 编辑成不同的image 
docker buildx build -o type=docker --platform linux/amd64 -t name:1.0.0 .
docker buildx build -o type=docker --platform linux/arm64 -t name:1.0.0-linux-arm64 .
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;cross-compilation&#34;&gt;cross-compilation&lt;/h3&gt;
&lt;hr&gt;
&lt;h2 id=&#34;overlay&#34;&gt;Overlay&lt;/h2&gt;
&lt;p&gt;最下层是一个 lower 层，也就是镜像层，它是一个只读层；&lt;/p&gt;
&lt;p&gt;右上层是一个 upper 层，upper 是容器的读写层，upper 层采用了写实复制的机制，也就是说只有对某些文件需要进行修改的时候才会从 lower 层把这个文件拷贝上来，之后所有的修改操作都会对 upper 层的副本进行修改；&lt;/p&gt;
&lt;p&gt;upper 并列的有一个 workdir，它的作用是充当一个中间层的作用。也就是说，当对 upper 层里面的副本进行修改时，会先放到 workdir，然后再从 workdir 移到 upper 里面去，这个是 overlay 的工作机制；&lt;/p&gt;
&lt;p&gt;最上面的是 mergedir，是一个统一视图层。从 mergedir 里面可以看到 upper 和 lower 中所有数据的整合，然后我们 docker exec 到容器里面，看到一个文件系统其实就是 mergedir 统一视图层。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 挂载到overlay
mount -t overlay -o lowerdir=/path/lower,upperdir=/path/upper,workdir=/path/work overlay /path/di
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;scratch&#34;&gt;scratch&lt;/h2&gt;
&lt;p&gt;scratch是空白镜像，一般用于基础镜像构建.比如制作alpine/ubuntu/debian/busybox镜像.&lt;/p&gt;
&lt;h2 id=&#34;ubuntudebian&#34;&gt;ubuntu/debian&lt;/h2&gt;
&lt;p&gt;Hash Sum mismatch:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RUN set -ex \
&amp;amp;&amp;amp; apt-get clean \
&amp;amp;&amp;amp; apt-get update -o Acquire::CompressionTypes::Order::=gz \
&amp;amp;&amp;amp; apt-get update \
&amp;amp;&amp;amp; apt-get install -y --allow-unauthenticated --no-install-recommends \
build-essential \
&amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/*
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;centosfedora&#34;&gt;centos/fedora&lt;/h2&gt;
&lt;h2 id=&#34;buildpack-deps&#34;&gt;buildpack-deps&lt;/h2&gt;
&lt;p&gt;在ubuntu/debian基础上安装一些工具，比ubuntu/debian镜像更大.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/docker-library/buildpack-deps&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/docker-library/buildpack-deps&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;busybox&#34;&gt;busybox&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/docker-library/busybox&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/docker-library/busybox&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;alpine推荐&#34;&gt;alpine(推荐)&lt;/h2&gt;
&lt;p&gt;很多语言的都是基于alpine: python-version:alpine-version, golang-version:alpine-version.&lt;/p&gt;
&lt;p&gt;一个基于musl和busybox的linux发行版.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.alpinelinux.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.alpinelinux.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;alpine比distroless尺寸大，包含包管理和shell，方便调试.&lt;/p&gt;
&lt;p&gt;alpine使用musl代替glibc会导致有的程序无法运行, 解决:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir /lib64
# ln -s /lib/libc.musl-x86_64.so.1 /lib64/ld-linux-x86-64.so.2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注意wget 部分参数不可用.&lt;/p&gt;
&lt;h2 id=&#34;distroless&#34;&gt;distroless&lt;/h2&gt;
&lt;p&gt;google提供的只包含运行时的精简镜像.&lt;/p&gt;
&lt;p&gt;缺点是没有包管理和shell，不方便调试.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/GoogleContainerTools/distroless&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/GoogleContainerTools/distroless&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;slim&#34;&gt;slim&lt;/h2&gt;
&lt;p&gt;减小image大小,适用于web程序.&lt;/p&gt;
&lt;p&gt;&amp;lt;https://github.com/docker-slim/docker-slim&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>Helm</title>
        <link>https://canuxcheng.com/post/k8s_helm/</link>
        <pubDate>Thu, 05 Sep 2019 22:02:31 +0000</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_helm/</guid>
        <description>&lt;h1 id=&#34;helm&#34;&gt;Helm&lt;/h1&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/helm/helm&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/helm/helm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;helm2有两个组件:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;helm: 客户端&lt;/li&gt;
&lt;li&gt;tiller: 服务端(helm3被移除)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;概念:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;chart: helm包，包含运行一个应用所需的镜像，依赖和资源.&lt;/li&gt;
&lt;li&gt;repository: 用于发布和存储chart的仓库.&lt;/li&gt;
&lt;li&gt;release: 在k8s集群上运行的一个chart实例.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;repository有哪些:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;charts: OSS, public, 已经被artifacthub取代, &lt;a class=&#34;link&#34; href=&#34;https://github.com/helm/charts&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/helm/charts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;artifacthub: OSS, public,  &lt;a class=&#34;link&#34; href=&#34;https://artifacthub.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://artifacthub.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;chartmuseum: OSS, private, self-host, &lt;a class=&#34;link&#34; href=&#34;https://github.com/helm/chartmuseum&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/helm/chartmuseum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;harbor: OSS, private, self-host.&lt;/li&gt;
&lt;li&gt;artifactory(jfrog): enterprise, private, self-host.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;
&lt;p&gt;helm和kubectl一样，访问指定配置的k8s集群。&lt;/p&gt;
&lt;p&gt;helm2需要安装tiller并且执行helm init初始化,helm不需要tiller.&lt;/p&gt;
&lt;p&gt;本地二进制安装helm3+:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl https://baltocdn.com/helm/signing.asc | sudo apt-key add -
sudo apt-get install apt-transport-https --yes
echo &amp;quot;deb https://baltocdn.com/helm/stable/debian/ all main&amp;quot; | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;命令&#34;&gt;命令&lt;/h2&gt;
&lt;h3 id=&#34;repo管理&#34;&gt;repo管理&lt;/h3&gt;
&lt;p&gt;查看有哪些repo,默认没有repo&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ helm repo list 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;添加repo并命名&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ helm repo add [NAME] [URL] [flags]

// 添加charts/artifactoryhub stable命名为stable
$ helm repo add stable https://charts.helm.sh/stable
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更新repo:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ helm repo update
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;chart管理&#34;&gt;chart管理&lt;/h3&gt;
&lt;p&gt;从repo中查找chart:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ helm search hub [KEYWORD] [flags]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;从repo里面的chart里面搜索关键字:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ helm search repo [keyword] [flags]

//查看repo里面所有chart
$ helm search repo

// 查看repo里面所有treafik chart
$ helm search repo traefik
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看chart信息:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm show chart traefik/traefik
helm show values traefik/traefik &amp;gt; value.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下载chart:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm pull chart-name
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;release管理&#34;&gt;release管理&lt;/h3&gt;
&lt;p&gt;install/upgrade/uninstall/rollback&lt;/p&gt;
&lt;p&gt;status/list/get/history&lt;/p&gt;
&lt;p&gt;安装chart:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm install [name] [chart] [flags]

// 验证签名
helm install --verify ...

helm install traefik traefik/traefik

// 需提前创建namespace, 修改value
helm install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard -f value.yaml -n kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看release(每个install都是一个release):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm list
-a, --all                  show all releases without any filter applied.
-A, --all-namespaces       list releases across all namespaces.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;卸载release:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm uninstall kubernetes-dashboard -n kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看状态:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm status mysql
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;获取release信息:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm get manifest [name]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;misc&#34;&gt;misc&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;helm plugin
helm env
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;chart开发&#34;&gt;chart开发&lt;/h2&gt;
&lt;p&gt;创建chart:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm create my-chart
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;debug chart：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 检查语法
helm lint my-chart

// 模拟安装
helm install my-chart ./my-chart --dry-run --debug
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;模板管理:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm template [name] [chart]

// 在本地渲染模板
helm template my-chart ./my-chart
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;依赖chart管理:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm dependency

// 将依赖的chart下载到chart目录.
helm dep up &amp;lt;chart-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;测试chart:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm test
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;打包chart:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://helm.sh/zh/docs/topics/provenance/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://helm.sh/zh/docs/topics/provenance/&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm package my-chart

// 会另外生成一个.prov文件
helm package --sign --key &amp;quot;Canux&amp;quot; --keyring ~/.gnupg/secring.gpg my-chart
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;验证打包的chart:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm verify my-chart.tgz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;生成index文件:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm repo index my-chart-folder --url &amp;lt;repo-url&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上传chart到registry：&lt;/p&gt;
&lt;p&gt;有的registry支持ui上直接upload，或者CLI上传，也可以通过helm push上传.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm registry login
helm push my-chart.tgz oci://registry/helm-charts
helm registry logout 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果是OCI registry目前只支持以下命令：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm registry login/logout
helm push
helm pull
helm show
helm template
helm install
helm upgrade
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;chart 目录:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;values.yaml
Chart.yaml
charts/
templates/
templates/tests/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Chart.yaml语法:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://helm.sh/zh/docs/topics/charts/#chartyaml-%E6%96%87%E4%BB%B6&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://helm.sh/zh/docs/topics/charts/#chartyaml-%E6%96%87%E4%BB%B6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;template的流控制:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://helm.sh/zh/docs/chart_template_guide/control_structures/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://helm.sh/zh/docs/chart_template_guide/control_structures/&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 用$代替. 来获取上一级的变量
{{- range .Values.worker }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include &amp;quot;app.fullname&amp;quot; $ }}-worker-{{ .name }}
labels:
    {{- include &amp;quot;worker.labels&amp;quot; $ | nindent 4 }}
spec:
  {{- if not .autoscaling.enabled }}
  replicas: {{ $.Values.replicaCount }}
  {{- end }}
  ......
---
{{- end }}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;template的函数:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://helm.sh/zh/docs/chart_template_guide/function_list/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://helm.sh/zh/docs/chart_template_guide/function_list/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;hooks:&lt;/p&gt;
&lt;p&gt;helm uninstall 不会删除有hook的资源。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://helm.sh/zh/docs/topics/charts_hooks/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://helm.sh/zh/docs/topics/charts_hooks/&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    &amp;quot;helm.sh/hook&amp;quot;: post-install, pre-install, ...
    &amp;quot;helm.sh/hook-weight&amp;quot;: &amp;quot;-5&amp;quot;
    &amp;quot;helm.sh/hook-delete-policy&amp;quot;: hook-succeeded, hook-failed, ...
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>K8S CRI</title>
        <link>https://canuxcheng.com/post/k8s_cri/</link>
        <pubDate>Wed, 05 Jun 2019 22:02:31 +0000</pubDate>
        
        <guid>https://canuxcheng.com/post/k8s_cri/</guid>
        <description>&lt;h1 id=&#34;cri&#34;&gt;CRI&lt;/h1&gt;
&lt;p&gt;CRI: Container Runtime Intarface&lt;/p&gt;
&lt;p&gt;定义了k8s和container runtime进行交互的接口.&lt;/p&gt;
&lt;p&gt;是k8s与container交互的标准.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;containerd&lt;/li&gt;
&lt;li&gt;cri-o&lt;/li&gt;
&lt;li&gt;rkt&lt;/li&gt;
&lt;li&gt;kata&lt;/li&gt;
&lt;li&gt;rancher&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;docker&#34;&gt;docker&lt;/h2&gt;
&lt;p&gt;k8s_1.20 开始警告不再支持docker.&lt;/p&gt;
&lt;p&gt;k8s_1.23 开始移除dockershim.&lt;/p&gt;
&lt;p&gt;/var/run/dockerhsim.sock&lt;/p&gt;
&lt;h2 id=&#34;containerd&#34;&gt;containerd&lt;/h2&gt;
&lt;p&gt;/run/container/containerd.sock&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/containerd/containerd&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/containerd/containerd&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;安装配置:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#containerd&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#containerd&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;cri-o&#34;&gt;CRI-O&lt;/h2&gt;
&lt;p&gt;redhat.&lt;/p&gt;
&lt;p&gt;/var/run/crio/crio.sock&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/cri-o/cri-o&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/cri-o/cri-o&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;kata&#34;&gt;kata&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kata-containers/runtime&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kata-containers/runtime&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;rkt&#34;&gt;rkt&lt;/h2&gt;
&lt;p&gt;redhat(coreos)&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/rkt/rkt&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/rkt/rkt&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>Container</title>
        <link>https://canuxcheng.com/post/cncf_container/</link>
        <pubDate>Thu, 05 Apr 2018 22:02:31 +0000</pubDate>
        
        <guid>https://canuxcheng.com/post/cncf_container/</guid>
        <description>&lt;h1 id=&#34;container&#34;&gt;Container&lt;/h1&gt;
&lt;p&gt;OCI: Open Container Initiative.&lt;/p&gt;
&lt;p&gt;CRI: Container Runtime Interface.&lt;/p&gt;
&lt;p&gt;CNI: Container Network Interface.&lt;/p&gt;
&lt;p&gt;CSI: Container Storage Interface.&lt;/p&gt;
&lt;h1 id=&#34;oci&#34;&gt;OCI&lt;/h1&gt;
&lt;p&gt;Open Container Initiative，也就是常说的OCI，是由多家公司共同成立的项目，并由linux基金会进行管理，致力于container runtime的标准的制定和runc的开发等工作.&lt;/p&gt;
&lt;p&gt;是container的标准.&lt;/p&gt;
&lt;p&gt;目前主要有两个标准文档：容器运行时标准 （runtime spec）和 容器镜像标准（image spec）&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.opencontainers.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.opencontainers.org/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;runc&#34;&gt;runc&lt;/h2&gt;
&lt;p&gt;docker(libcontainer)&lt;/p&gt;
&lt;p&gt;runc支持OCI.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/opencontainers/runc&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/opencontainers/runc&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;rkt&#34;&gt;rkt&lt;/h2&gt;
&lt;p&gt;redhat(coreos)&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/rkt/rkt&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/rkt/rkt&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;tools&#34;&gt;tools&lt;/h1&gt;
&lt;h2 id=&#34;dumb-init&#34;&gt;dumb-init&lt;/h2&gt;
&lt;p&gt;管理pid=1的进程的子进程:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/Yelp/dumb-init&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/Yelp/dumb-init&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;baseimage-docker&#34;&gt;baseimage-docker&lt;/h2&gt;
&lt;p&gt;处理container中运行多个进程的问题:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/phusion/baseimage-docker&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/phusion/baseimage-docker&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;watchtower&#34;&gt;watchtower&lt;/h2&gt;
&lt;p&gt;根据registry中的更新自动更新 container:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/containrrr/watchtower/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/containrrr/watchtower/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;hadolint&#34;&gt;hadolint&lt;/h2&gt;
&lt;p&gt;dockerfile 语法检查:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/hadolint/hadolint&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/hadolint/hadolint&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;gosu&#34;&gt;gosu&lt;/h2&gt;
&lt;p&gt;权限管理&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/tianon/gosu&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/tianon/gosu&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;swarm-cronjob&#34;&gt;swarm-cronjob&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/crazy-max/swarm-cronjob&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/crazy-max/swarm-cronjob&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;misc&#34;&gt;misc&lt;/h1&gt;
&lt;p&gt;get host ip(docker/docker_gwbridge) from container:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ip route | awk &#39;/default/ { print $3 }&#39;
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Docker</title>
        <link>https://canuxcheng.com/post/cncf_docker/</link>
        <pubDate>Thu, 05 Apr 2018 22:02:31 +0000</pubDate>
        
        <guid>https://canuxcheng.com/post/cncf_docker/</guid>
        <description>&lt;h1 id=&#34;docker&#34;&gt;Docker&lt;/h1&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://store.docker.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://store.docker.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://hub.docker.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://hub.docker.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/docker&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/docker&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/moby/moby&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/moby/moby&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Docker是一个容器引擎, 分为社区版CE, 和企业版EE, Docker不是虚拟机, 也不依赖虚拟化技术．&lt;/p&gt;
&lt;p&gt;docker-cli -&amp;gt; dockerd -&amp;gt; containerd -&amp;gt; shim -&amp;gt; runc&lt;/p&gt;
&lt;p&gt;containerd是容器运行时管理引擎.&lt;/p&gt;
&lt;p&gt;shim用于管理容器生命周期.&lt;/p&gt;
&lt;p&gt;Docker包括三个基本概念:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;仓库repository,集中存放镜像文件的场所,docker hub/store是最大的公开仓库．&lt;/li&gt;
&lt;li&gt;镜像image, 镜像是一个文件系统.&lt;/li&gt;
&lt;li&gt;容器container, 容器是镜像的运行的实例．&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;修改docker存储路径:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ service docker stop
$ mv /var/lib/docker /opt/ssd/docker
$ ln -s /opt/ssd/docker /var/lib/docker
$ service docker start
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Install:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;windows:
&amp;lt;https://docs.docker.com/docker-for-windows/install/&amp;gt;

linux:
&amp;lt;https://docs.docker.com/engine/install/ubuntu/&amp;gt;

mac:
&amp;lt;https://github.com/abiosoft/colima/blob/main/docs/FAQ.md#docker&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;config&#34;&gt;config&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;docker配置文件:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/etc/docker/daemon.json
/lib/systemd/system/docker.service

{
    // debug
    &amp;quot;debug&amp;quot;: true,
    
    &amp;quot;data-root&amp;quot;： &amp;quot;/var/lib/docker&amp;quot;,
    
    &amp;quot;features&amp;quot;: {
        &amp;quot;buildkit&amp;quot;: true
    },

    //容器访问外网:
    ip-forward=true 会设置 net.ipv4.ip_forward=1, 才能访问外网
    // 容器之间访问:
    icc=true, 
    iptables=true  会修改iptables的forward策略为accept,

    // 修改默认docker0
    &amp;quot;bridge&amp;quot;:
    &amp;quot;bip&amp;quot;: &amp;quot;10.0.0.1/16&amp;quot;  // subnet + gateway
    &amp;quot;fixed-cidr&amp;quot;: &amp;quot;10.41.0.0/24&amp;quot; // iprange
    &amp;quot;fixed-cidr-v6&amp;quot;: &amp;quot;&amp;quot;,
    &amp;quot;mtu&amp;quot;: 1500
    &amp;quot;default-gateway&amp;quot;:
	&amp;quot;default-gateway-v6&amp;quot;: &amp;quot;&amp;quot;,

    // 修改默认dns
    &amp;quot;dns&amp;quot; : [
        &amp;quot;114.114.114.114&amp;quot;,
        &amp;quot;8.8.8.8&amp;quot;
    ]
    &amp;quot;dns-opts&amp;quot;: [],
    &amp;quot;dns-search&amp;quot;: [],

    // ipv6
    &amp;quot;ipv6&amp;quot;: true

    // private registry
  	&amp;quot;insecure-registries&amp;quot;: [],

    // 修改registry
    &amp;quot;registry-mirrors&amp;quot;: [
        &amp;quot;https://registry.docker-cn.com&amp;quot;,
        &amp;quot;https://z4yd270h.mirror.aliyuncs.com&amp;quot;,
        &amp;quot;http://f1361db2.m.daocloud.io&amp;quot;,
        &amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;
    ]

    &amp;quot;hosts&amp;quot;: [],
    &amp;quot;log-level&amp;quot;: &amp;quot;&amp;quot;,
    &amp;quot;tls&amp;quot;: true,
    &amp;quot;tlsverify&amp;quot;: true,
    &amp;quot;tlscacert&amp;quot;: &amp;quot;&amp;quot;,
    &amp;quot;tlscert&amp;quot;: &amp;quot;&amp;quot;,
    &amp;quot;tlskey&amp;quot;: &amp;quot;&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;proxy for pull image from google(gcr):&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.docker.com/config/daemon/systemd/#httphttps-proxy&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://docs.docker.com/config/daemon/systemd/#httphttps-proxy&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;docker命令&#34;&gt;docker命令&lt;/h2&gt;
&lt;p&gt;system:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker system df	// Show docker disk usage
$ docker system events	// Get real time events from the server
$ docker system info	// Display system-wide information
$ docker system prune	// Remove unused data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;image管理:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker image COMMAND

&amp;gt; 查看本地镜像:
$ docker image ls
$ docker image -a

&amp;gt; 根据创建dockerfile，创建新的images
$ docker image build

&amp;gt; 创建tag
$ docker image tag

&amp;gt; 删除image
$ docker image rm &amp;lt;IMAGE ID&amp;gt;
$ docker rmi &amp;lt;IMAGE ID&amp;gt;
&amp;gt; 删除所有image
$ docker rmi $(docker images -a -q)

&amp;gt; 清理所有临时images
$ docker image prune
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;container管理&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$docker container COMMAND

# 列出container:
$ docker container ls
$ docker ps -a  # 默认只显示running状态的

# 运行image,产生一个container:
$ docker container run &amp;lt;IMAGE ID&amp;gt;/&amp;lt;REPOSITORY&amp;gt; [COMMAND] [ARGS]

# 在container中执行命令
$ docker container exec [OPTIONS] &amp;lt;CONTAINER&amp;gt; COMMAND [ARG...]

# 创建container但不启动
$ docker container create --name &amp;lt;name&amp;gt; &amp;lt;CONTAINER&amp;gt;

# 启动container:
$ docker container start/restart &amp;lt;CONTAINER&amp;gt;

# 停止container:
$ docker container stop &amp;lt;CONTAINER&amp;gt;

# 删除container：
$ docker container rm &amp;lt;CONTAINER&amp;gt;
$ docker rm &amp;lt;CONTAINER&amp;gt;
# 删除所有容器
$ docker rm $(docker ps -a -q) 

# 清理停止的container
$ docker container prune
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;制作镜像builder&lt;/p&gt;
&lt;p&gt;build: deprecated.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 根据Dockerfile 构建image
docker builder [OPTIONS] PATH | URL | -
docker builder build .  // 默认就是当前目录的Dockerfile
docker builder build -t &amp;lt;repo&amp;gt;/&amp;lt;name&amp;gt;:&amp;lt;tag&amp;gt; .  // 创建tag
docker builder build -f /path/to/mydockerfile . // 也可以指定其它路径的其它文件
docker builder build --target &amp;lt;stage&amp;gt; . // 指定阶段构建.
docker builder build ... --network=host // 使用host网络构建.
docker builder build --no-cache // 不使用缓存数据
--cache-from // 使用本地或registry上的cache
--cache-to // cache存到本地或registry
--progress = auto/plain/tty,  // plain 显示更多log。 

docker builder prune --all // 清理所有image 的缓存。

# 把image导出到tar包
# 既可以从image也可以从container导出。
# 从container导出不包含运行后的修改，只导出原始镜像。
$ docker save -o name.tgz &amp;lt;repo1&amp;gt;:&amp;lt;tag1&amp;gt; &amp;lt;repo2&amp;gt;:&amp;lt;tag2&amp;gt; ...

# 从stdin或文件加载image
docker load [OPTIONS]
docker load &amp;lt; name.tar.gz
docker load --input/-i name.tar

# 把container导出到tar包，从container导出镜像。
# 包括container启动后的修改。
docker export -o name.tar [container]

# 从container导出的包加载成镜像
docker import name.tar [repo]:[tag]

# 根据container的修改创建新的image
docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]
docker commit -a &amp;quot;author&amp;quot; -c &amp;quot;Dockerfile instruction&amp;quot; -m &amp;quot;commit message&amp;quot; CONTAINER [REPOSITORY[:TAG]]

# 创建新的tag
docker tag &amp;lt;old&amp;gt; &amp;lt;new&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;运行容器&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 创建一个新的container并运行命令
docker run [OPTIONS] IMAGE [COMMAND] [ARG...]
docker run --name [NAME] IMAGE // 运行容器并命名
docker run -d IMAGE // 后台运行
docker run -it IMAGE /bin/bash // 交互模式启动容器
docker run -P IMAGE // 默认将容器的8０端口映射到主机的随机端口
docker run -p [host:port]:[containerPort] // 指定映射端口
--add-host # 相当于修改容器的/etc/hosts,但是容器重启后不会消失
-h/--hostname # 修改容器的/etc/hostname

// cpu
--cpus decimal                   Number of CPUs
-c, --cpu-shares int             CPU shares (relative weight)
--cpuset-cpus string             CPUs in which to allow execution (0-3, 0,1)
--cpuset-mems string             MEMs in which to allow execution (0-3, 0,1)
--cpu-period
--cpu-quota

// memory
-m, --memory bytes               Memory limit
--memory-reservation bytes       Memory soft limit
--memory-swap bytes              Swap limit equal to memory plus swap: &#39;-1&#39; to enable unlimited swap
--memory-swappiness int          Tune container memory swappiness (0 to 100) (default -1)
--oom-kill-disable
--oom-score-adj
--kernel-memory

// io
--blkio-weight
--blkio-weight-device
--device-read-bps
--device-write-bps
--device-read-iops
--device-write-iops

// security
// https://docs.docker.com/engine/security/apparmor/
// https://docs.docker.com/engine/security/seccomp/
// https://docs.docker.com/engine/security/userns-remap/
// https://docs.docker.com/engine/security/rootless/
privileged
sysctl
ulimit
user
userns
security_ops
cgroup_parent
cap_add
cap_drop

# 在运行的container中执行命令
docker exec [OPTIONS] CONTAINER COMMAND [ARG...]
docker exec -d CONTAINER ... // 在后台运行
docker exec -it CONTAINER /bin/bash ... // 进入命令行

# metrics
docker stats

# resource
docker top
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;log:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 查看log driver
// 默认driver是 json-file, 不同的driver，option不同
// driver是json-file, journald, 通过docker logs docker-compose logs 才能看到log
$ docker inspect -f &#39;{{.HostConfig.LogConfig.Type}}&#39; &amp;lt;ID&amp;gt;

docker run -it --log-driver &amp;lt;driver&amp;gt; --log-opt mode=blocking --log-opt max-buffer-size=4m alpine ash  
// --log-driver json-file
// --log-opt mode blocking/non-blocking
// --log-opt max-buffer-size 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其它命令&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 在container和host之前拷贝文件和目录
docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|-
docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH

# 查看容器的日志
docker logs [OPTIONS] CONTAINER
docker diff CONTAINER
docker history [OPTIONS] IMAGE

# 查看healthcheck的log
docker inspect --format &amp;quot;{{json .State.Health }}&amp;quot; &amp;lt;container name&amp;gt; | jq

docker system prune
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;register使用&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 登陆到docker hub 或其他register
docker login
docker login -u/--username &amp;lt;user&amp;gt; -p/--password &amp;lt;password&amp;gt;

# 从docker hub/store查找images
$ docker search [OPTIONS] TERM
$ docker search

# 从registry获取repository/images到/var/lib/docker：
$ docker pull [OPTIONS] NAME[:TAG|@DIGEST]
$ docker pull
$ docker pull ubuntu # 默认下载所有tag
$ docker pull ubuntu:14.04

# 从中国站点下载
$ docker pull registry.docker-cn.com/library/ubuntu:16.04

# 推送到docker hub
$ docker push
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;secret&lt;/p&gt;
&lt;p&gt;secret以文件形式存在于/run/secrets/&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 保存敏感数据
$ docker secret

docker secret ls
docker secret inspect &amp;lt;ID/name&amp;gt;
docker secret rm &amp;lt;id/name&amp;gt;

// 根据文件创建
docker secret create &amp;lt;s-name&amp;gt; &amp;lt;s-file&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;config&lt;/p&gt;
&lt;p&gt;config以文件形式存在于/&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 保存非敏感数据
$ docker config

docker config ls
docker config inspect &amp;lt;id/name&amp;gt;
docker config rm &amp;lt;id/name&amp;gt;

docker config create &amp;lt;c-name&amp;gt; &amp;lt;c-file&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;plugin&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 安装plugin
docker plugin install --grant-all-permissions name:tag key=value key=value
// 修改plugin的参数
docker plugin set name:tag key=value
// 查看plugin的参数，settable里面的是改参数能修改的变量
docker plugin inspect name:tag
// 激活plugin
docker plugin enable name:tag
// 删除
docker plugin rm name:tag
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;开发插件:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker plugin create name:tag ./plugin
docker plugin push name:tag
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dockerfile&#34;&gt;Dockerfile&lt;/h2&gt;
&lt;p&gt;每个命令都会创建一个layer,尽可能合并相同的命令。&lt;/p&gt;
&lt;p&gt;ADD&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# add可以是远程文件，不推荐使用
ADD [--chown=&amp;lt;user&amp;gt;:&amp;lt;group&amp;gt;] &amp;lt;src&amp;gt;... &amp;lt;dest&amp;gt;
ADD [--chown=&amp;lt;user&amp;gt;:&amp;lt;group&amp;gt;] [&amp;quot;&amp;lt;src&amp;gt;&amp;quot;,... &amp;quot;&amp;lt;dest&amp;gt;&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;COPY&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 只能操作本地文件, 目标路径不需要创建，不存在会自动创建.
COPY [--chown=&amp;lt;user&amp;gt;:&amp;lt;group&amp;gt;] &amp;lt;src&amp;gt;... &amp;lt;dest&amp;gt;
COPY [--chown=&amp;lt;user&amp;gt;:&amp;lt;group&amp;gt;] [&amp;quot;&amp;lt;src&amp;gt;&amp;quot;,... &amp;quot;&amp;lt;dest&amp;gt;&amp;quot;]
COPY file /path/to/file  相当于 COPY file /path
# 不会创建目录
COPY folder /path 相当于 COPY folder/* /path/
# 需要手动指定
COPY folder  /path/to/folder 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ENV&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;env会被派生image继承。

# 指定容器中的环境变量
ENV &amp;lt;key&amp;gt; &amp;lt;value&amp;gt;
ENV &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt; ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;EXPOSE&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;expose会被派生image继承.
# 指定容器需要映射到主机的端口
EXPOSE &amp;lt;port&amp;gt; [&amp;lt;port&amp;gt;/&amp;lt;protocol&amp;gt;...]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;FROM&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM &amp;lt;image&amp;gt; [AS &amp;lt;name&amp;gt;]
FROM &amp;lt;image&amp;gt;[:&amp;lt;tag&amp;gt;] [AS &amp;lt;name&amp;gt;]
FROM &amp;lt;image&amp;gt;[@&amp;lt;digest&amp;gt;] [AS &amp;lt;name&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;LABEL&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;label会被派生image继承.
LABEL &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt; &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt; &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt; ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;STOPSIGNAL&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;STOPSIGNAL signal
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;USER&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;USER &amp;lt;user&amp;gt;[:&amp;lt;group&amp;gt;]
USER &amp;lt;UID&amp;gt;[:&amp;lt;GID&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;VOLUME&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 指定挂载点做数据持久化
VOLUME [&amp;quot;/data&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;WORKDIR&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;workdir会被派生image继承.
# 指定后的操作都以该目录为当前目录，目录不存在会自动创建
WORKDIR /path/to/workdir
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;RUN&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 在容器构建过程中运行
RUN &amp;lt;command&amp;gt;  # shell 格式
RUN [&amp;quot;executable&amp;quot;, &amp;quot;param1&amp;quot;, &amp;quot;param2&amp;quot;]   # exec 格式

RUN set -ex \
&amp;amp;&amp;amp; cmd1 \
&amp;amp;&amp;amp; cmd2......
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ENTRYPOINT&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;entrypoint会被派生image继承，除非你在派生image重新指定.
# 和cmd类似,指定容器运行过程中的执行命令
ENTRYPOINT [&amp;quot;executable&amp;quot;, &amp;quot;param1&amp;quot;, &amp;quot;param2&amp;quot;] (exec form, preferred)
ENTRYPOINT command param1 param2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;CMD&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cmd会被派生image继承，除非你在派生image重新指定了entrypoint.
# 在容器运行过程中运行, 可以被覆盖
CMD [&amp;quot;executable&amp;quot;, &amp;quot;param1&amp;quot;, &amp;quot;param2&amp;quot;]  # exec格式
CMD command param1 param2   # shell 格式,通过/bin/bash 或 /bin/sh 执行.
// 同时有cmd和entrypoint,cmd只是entrypoint的参数.
CMD [&amp;quot;param2&amp;quot;, &amp;quot;param2&amp;quot;]   # entrypoint的参数
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ARG&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 指定构建环境的变量
ARG &amp;lt;name&amp;gt;[=&amp;lt;default value&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ONBUILD&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;onbuild会被派生image继承.
ONBUILD ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;stage:&lt;/p&gt;
&lt;p&gt;多级构建，一般0级用来编译源代码，并且组织目录结构。
下一级直接将编译好的或者组织好的目录结构copy到指定位置。
这样最终的image不会含有源代码的overlay.
也不会有临时目录结构的overlay.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 不指定stage name, 默认就是数字
COPY --from=0 /path/folder /path/folder
COPY --from=stage1 /path/folder /path/folder 
docker build --target stage1 -t docker:latest . 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;.dockerignore &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# comment
/path/folder
path/folder
*/tmp*
*/*/tmp*
tmp?
*.md
!README.md  
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;docker-machine&#34;&gt;docker-machine&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/docker/machine&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/docker/machine&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在本地安装docker和docker-machine，然后就可以从本机安装或管理远程机器上的docker.&lt;/p&gt;
&lt;p&gt;需要添加ssh的无密码登陆:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh-copy-id -i ~/.ssh/id_rsa.pub user@remote-ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;安装:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://docs.docker.com/machine/install-machine/$ base=https://github.com/docker/machine/releases/download/v0.16.0
&amp;amp;&amp;amp; curl -L $base/docker-machine-$(uname -s)-$(uname -m) &amp;gt;/tmp/docker-machine
&amp;amp;&amp;amp; sudo install /tmp/docker-machine /usr/local/bin/docker-machine
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;docker-machine命令&#34;&gt;docker-machine命令&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;docker-machine create -d generic \
--generic-ip-address=remote-ip \
--generic-ssh-user=user \
--generic-ssh-key ~/.ssh/id_rsa \
node1
docker-machine ls
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;oom&#34;&gt;OOM&lt;/h2&gt;
&lt;p&gt;kernel OOM 会导致kernel随机kill一些container释放内存。&lt;/p&gt;
&lt;p&gt;防止kernel OOM的方法就是设置resource limit.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.docker.com/config/containers/resource_constraints/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://docs.docker.com/config/containers/resource_constraints/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/docker/compose/issues/4513&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/docker/compose/issues/4513&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;swarm mode:&lt;/p&gt;
&lt;p&gt;swarm模式使用compose format 3来限制资源.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.docker.com/compose/compose-file/#resources&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://docs.docker.com/compose/compose-file/#resources&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;non swarm mode:&lt;/p&gt;
&lt;p&gt;非swarm模式可以用compose format 2 来做资源限制.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.docker.com/compose/compose-file/compose-file-v2/#cpu-and-other-resources&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://docs.docker.com/compose/compose-file/compose-file-v2/#cpu-and-other-resources&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;colima&#34;&gt;colima&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/abiosoft/colima&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/abiosoft/colima&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;docker for mac&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;brew install colima docker
colima start
brew services restart colima
sudo ln -sf /Users/canche01/.colima/default/docker.sock /var/run/docker.sock
docker ps -a
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Kubernetes</title>
        <link>https://canuxcheng.com/post/kubernetes/</link>
        <pubDate>Thu, 05 Apr 2018 22:02:31 +0000</pubDate>
        
        <guid>https://canuxcheng.com/post/kubernetes/</guid>
        <description>&lt;h1 id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h1&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/kubernetes&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/kubernetes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/kubeadm&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/kubeadm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/kops&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes/kops&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes-sigs/kubespray&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes-sigs/kubespray&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;kubernetes简称k8s, 是开源的容器编排工具。&lt;/p&gt;
&lt;p&gt;安装单机版k8s:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;minikube&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;安装k8s集群:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kubeadm (k8s内置的，类似于docker swarm mode, 没有HA)&lt;/li&gt;
&lt;li&gt;kops (目前主要支持aws等云平台, 国内不友好)&lt;/li&gt;
&lt;li&gt;kubespray (通过ansible部署, 国内不友好)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;k8s发行版：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;openshift-okd(redhat)&lt;/li&gt;
&lt;li&gt;rancher&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;k8s集群组成&#34;&gt;k8s集群组成&lt;/h1&gt;
&lt;h2 id=&#34;master&#34;&gt;master&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;aip-server, 提供资源操作唯一入口&lt;/li&gt;
&lt;li&gt;scheduler, 负责资源调度&lt;/li&gt;
&lt;li&gt;controller-manager, 负责维护集群状态&lt;/li&gt;
&lt;li&gt;etcd(可以部署单独集群), 保存整个集群的状态&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;node&#34;&gt;node&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;kubelet, 负责维护容器生命周期, 还包括CNI CVI&lt;/li&gt;
&lt;li&gt;kube-proxy, 为service提供cluster内部的服务发现和负载均衡&lt;/li&gt;
&lt;li&gt;CRI(containerd), 创建pod&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;addons&#34;&gt;addons&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;coredns&lt;/li&gt;
&lt;li&gt;flannel/cilium/calico&lt;/li&gt;
&lt;li&gt;dashboard, web-gui&lt;/li&gt;
&lt;li&gt;metrics-server, 取代heapster，用于cpu/memory监控&lt;/li&gt;
&lt;li&gt;ingress-nginx, 为服务提供外网入口&lt;/li&gt;
&lt;li&gt;federation, 提供跨可用区的集群&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;概念&#34;&gt;概念&lt;/h1&gt;
&lt;p&gt;k8s包含的重要概念:&lt;/p&gt;
&lt;p&gt;-: nodes, 运行pod的物理机或虚拟机.
-: namespace, 对资源和对象的抽象集合．pods/deployments/services都属于某个ns.
-: pods,一组紧密关联的容器集合，共享pid,ipc,network,uts,namespace.&lt;/p&gt;
&lt;p&gt;k8s业务类型:&lt;/p&gt;
&lt;p&gt;-: long-running 长期伺服型 -&amp;gt; RC, RS, Deployment
-: batch 批处理型-&amp;gt; Job
-: node-daemon 节点后台支撑型-&amp;gt; DaemonSet
-: stateful application 有状态应用型-&amp;gt; StatefulSet&lt;/p&gt;
&lt;p&gt;api对象三大类属性:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;metadata 元数据(至少包含namespace, name, uid).&lt;/li&gt;
&lt;li&gt;spec 规范&lt;/li&gt;
&lt;li&gt;status 状态&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;kubernetes对外暴露服务的三种方式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NodePort: dev/qa. (30000-32767)&lt;/li&gt;
&lt;li&gt;Ingress: production.&lt;/li&gt;
&lt;li&gt;LoadBalance: cloud provider.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;clusterIP 是k8s内部默认服务，外部无法访问，可以通过proxy来访问。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;pod&#34;&gt;pod&lt;/h1&gt;
&lt;p&gt;一个pod包含一个或多个容器，这些容器通过infra container共享同一个network namespace。&lt;/p&gt;
&lt;p&gt;infra container: k8s.gcr.io/pause, 汇编写的，永远处于暂停状态。&lt;/p&gt;
&lt;p&gt;pod共享网络:&lt;/p&gt;
&lt;p&gt;同一个pod里面看到的网络跟infra容器看到的是一样的，一个pod只有一个ip,也就是这个Pod的network namespace对应的IP; 整个pod的生命周期跟infra容易是一样的。&lt;/p&gt;
&lt;p&gt;pod共享存储:&lt;/p&gt;
&lt;p&gt;通过pod volumes, 使pod中的container共享存储。&lt;/p&gt;
&lt;p&gt;应用:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个pod中的某个容器异常退出，被kubelet拉起来之前保证之前的数据部丢失.&lt;/li&gt;
&lt;li&gt;同一个pod的多个容器共享数据.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;pod volume类型:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;本地存储: emptydir/hostpath&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;网络存储:&lt;/p&gt;
&lt;p&gt;in-tree(ks8内置支持): awsElasticBlockStore/gcePresistentDisk/nfs&amp;hellip;&lt;/p&gt;
&lt;p&gt;out-of-tree(插件): flexvolume/csi&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;projected volume: secret/configmap/downwardAPI/serviceAccountToken&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;pod服务质量配置:&lt;/p&gt;
&lt;p&gt;依据容器对cpu,memory资源的request/limit需求，pod服务质量分类:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Guaranteed&lt;/li&gt;
&lt;li&gt;Burstable&lt;/li&gt;
&lt;li&gt;BestEffort&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;initContainer:&lt;/p&gt;
&lt;p&gt;initContainer用于普通Container启动前的初始化（如配置文件准备) 和 前置条件校验 (如网络)。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;initContainer会先于普通container启动执行，直到所有initcontainer执行成功后，普通container 才会执行。&lt;/li&gt;
&lt;li&gt;pod中多个initcontainer之间是按次序依次启动执行，而pod中多个普通container是并行启动。&lt;/li&gt;
&lt;li&gt;initcontainer 执行成功后就结束退出，而普通container会一直执行或重启。&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;resources&#34;&gt;resources&lt;/h1&gt;
&lt;p&gt;查看所有对象:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl api-resources
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#statefulsetcondition-v1beta2-apps&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#statefulsetcondition-v1beta2-apps&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;workloads&#34;&gt;Workloads&lt;/h2&gt;
&lt;h3 id=&#34;replicasetsrs&#34;&gt;replicasets/rs&lt;/h3&gt;
&lt;p&gt;控制无状态的pod数量,增删Pods.&lt;/p&gt;
&lt;h3 id=&#34;deploymentsdeploy&#34;&gt;deployments/deploy&lt;/h3&gt;
&lt;p&gt;定义pod的数目和版本，通过Controller自动恢复失败的pod，滚动升级，重新生成，回滚等。&lt;/p&gt;
&lt;p&gt;DeploymentStatus: complete, processing, failed&lt;/p&gt;
&lt;p&gt;Deployment只负责管理不同版本的ReplicaSet, 由ReplicaSet管理Pod副本个数; 每个版本的ReplicaSet对应了Deployment template的一个版本; 一个ReplicaSet下的Pod都是相同的版本.&lt;/p&gt;
&lt;h3 id=&#34;daemonsetsds&#34;&gt;daemonsets/ds&lt;/h3&gt;
&lt;p&gt;ds作用:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;保证集群内每个节点运行一组相同的pod&lt;/li&gt;
&lt;li&gt;跟踪集群节点状态，保证新加入的节点自动创建对应的pod&lt;/li&gt;
&lt;li&gt;跟踪集群节点状态，保证移除的节点删除对应的pod&lt;/li&gt;
&lt;li&gt;跟踪pod状态，保证每个节点pod处于运行状态&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;jobs&#34;&gt;jobs&lt;/h3&gt;
&lt;p&gt;job的作用:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;创建一个或多个pod确保指定数量的Pod可以成功运行和终止.&lt;/li&gt;
&lt;li&gt;跟踪pod状态，根据配置及时重试失败的pod.&lt;/li&gt;
&lt;li&gt;确定依赖关系，保证上一个任务运行完毕后再运行下一个任务.&lt;/li&gt;
&lt;li&gt;控制任务并行度，并根据配置确保pod队列大小。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cronjobscj&#34;&gt;cronJobs/cj&lt;/h3&gt;
&lt;h3 id=&#34;statefulsetssts&#34;&gt;statefulsets/sts&lt;/h3&gt;
&lt;p&gt;控制有状态的pod数量.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;service-discovery--load-balancing&#34;&gt;service discovery &amp;amp; load balancing&lt;/h2&gt;
&lt;h3 id=&#34;servicessvc&#34;&gt;services/svc&lt;/h3&gt;
&lt;p&gt;提供访问一个或多个pod的稳定的访问地址.支持ClusterIP, NodePort, LoadBalancer等访问方式.&lt;/p&gt;
&lt;h3 id=&#34;ingresses&#34;&gt;ingresses&lt;/h3&gt;
&lt;h3 id=&#34;endpointslices&#34;&gt;endpointslices&lt;/h3&gt;
&lt;hr&gt;
&lt;h2 id=&#34;config--storage&#34;&gt;Config &amp;amp; Storage&lt;/h2&gt;
&lt;h3 id=&#34;secrets&#34;&gt;secrets&lt;/h3&gt;
&lt;p&gt;secret用于在集群中存储密码，token等敏感信息用的资源对象.&lt;/p&gt;
&lt;p&gt;其中敏感数据采用base-64编码保存.&lt;/p&gt;
&lt;p&gt;四种类型:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Opaque&lt;/li&gt;
&lt;li&gt;kubernetes.io/service-account-token&lt;/li&gt;
&lt;li&gt;kubernetes.io/dockerconfigjson&lt;/li&gt;
&lt;li&gt;bootstrap.kubernetes.io/token&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;secret主要被pod使用，一般通过volume挂载到指定容器目录，供容器中业务使用.&lt;/p&gt;
&lt;p&gt;访问私有镜像仓库也可以通过secret实现.&lt;/p&gt;
&lt;p&gt;secret大小限制1M; secret不适合机密信息，推荐用vault.&lt;/p&gt;
&lt;h3 id=&#34;configmapscm&#34;&gt;configmaps/cm&lt;/h3&gt;
&lt;p&gt;主要管理容器运行所需的配置文件，环境变量，命令行参数等可变配置。&lt;/p&gt;
&lt;p&gt;可用于解耦容器镜像和可变配置，从而保障工作负载的可移植性.&lt;/p&gt;
&lt;p&gt;主要被pod使用，一般用于挂载pod用的配置文件，环境变量，命令行参数.&lt;/p&gt;
&lt;p&gt;ConigMap 大小不超过1M; pod只能引用相同namespace中的configmap, pod引用的configmap不存在时，pod无法创建;使用envFrom从ConfigMap配置环境变量时，如果ConfigMap中的某些key被认为无效，该环境变量不会注入容器，但是pod可以创建; 只有通过k8s api创建的pod才能使用ConfigMap, 其它方式创建的pod不能使用ConfigMap.&lt;/p&gt;
&lt;h3 id=&#34;persistentvolumepv&#34;&gt;persistentvolume/pv&lt;/h3&gt;
&lt;p&gt;static volume provisioning:&lt;/p&gt;
&lt;p&gt;dynamic volume provisioning:&lt;/p&gt;
&lt;h3 id=&#34;persistentvolumeclaimspvc&#34;&gt;persistentvolumeclaims/pvc&lt;/h3&gt;
&lt;p&gt;pvc中只需要申明需要的存储size, access mode等业务需求.&lt;/p&gt;
&lt;p&gt;pvc简化了用户对存储的需求，pv才是存储的实际信息载体，通过kube-controller-manager中的PersistentVolumeController将PVC与合适的PV 绑定.&lt;/p&gt;
&lt;h3 id=&#34;csidrivers&#34;&gt;csidrivers&lt;/h3&gt;
&lt;h3 id=&#34;csinodes&#34;&gt;csinodes&lt;/h3&gt;
&lt;h3 id=&#34;storageclassessc&#34;&gt;storageclasses/sc&lt;/h3&gt;
&lt;h3 id=&#34;volumeattachments&#34;&gt;volumeattachments&lt;/h3&gt;
&lt;hr&gt;
&lt;h2 id=&#34;metadata&#34;&gt;metadata&lt;/h2&gt;
&lt;h3 id=&#34;events&#34;&gt;events&lt;/h3&gt;
&lt;h3 id=&#34;horizontalpodautoscaler&#34;&gt;horizontalpodautoscaler&lt;/h3&gt;
&lt;h3 id=&#34;podtemplate&#34;&gt;podtemplate&lt;/h3&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cluster&#34;&gt;cluster&lt;/h2&gt;
&lt;h3 id=&#34;componentstatusescs&#34;&gt;componentstatuses/cs&lt;/h3&gt;
&lt;h3 id=&#34;namespacesns&#34;&gt;namespaces/ns&lt;/h3&gt;
&lt;p&gt;一个集群内部的逻辑隔离机制，每个资源都属于一个namespace，同一个namespace中的资源命名唯一，不同namespace中的资源可重名.&lt;/p&gt;
&lt;h3 id=&#34;nodesno&#34;&gt;nodes/no&lt;/h3&gt;
&lt;h3 id=&#34;serviceaccountsa&#34;&gt;serviceaccount/sa&lt;/h3&gt;
&lt;p&gt;解决pod在集群中的身份认证问题.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;tools&#34;&gt;Tools&lt;/h1&gt;
&lt;p&gt;sonobuoy&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/vmware-tanzu/sonobuoy&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/vmware-tanzu/sonobuoy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;kuard&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes-up-and-running/kuard&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/kubernetes-up-and-running/kuard&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Namespace</title>
        <link>https://canuxcheng.com/post/cncf_ns/</link>
        <pubDate>Thu, 05 Apr 2018 22:02:31 +0000</pubDate>
        
        <guid>https://canuxcheng.com/post/cncf_ns/</guid>
        <description>&lt;h1 id=&#34;ns&#34;&gt;NS&lt;/h1&gt;
&lt;p&gt;namespace 是 Linux 内核用来隔离内核资源的方式&lt;/p&gt;
&lt;p&gt;Linux namespace资源：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mount: 挂载点&lt;/li&gt;
&lt;li&gt;Network: 网络设备/协议栈/端口&lt;/li&gt;
&lt;li&gt;IPC: 进程间通信&lt;/li&gt;
&lt;li&gt;USR: 用户和用户组&lt;/li&gt;
&lt;li&gt;PID: 进程&lt;/li&gt;
&lt;li&gt;UTS: 主机名和域名&lt;/li&gt;
&lt;li&gt;Cgroup: 控制组&lt;/li&gt;
&lt;li&gt;Time&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;cgroup&#34;&gt;Cgroup&lt;/h1&gt;
&lt;p&gt;Cgroup: Control Groups.是linux下用于对一个或一组进程资源控制和监控的机制.可以管理cpu, memory,diskIO 等资源.不同资源的具体管理工作由相应的cgrouup子系统来实现.&lt;/p&gt;
&lt;p&gt;cgroup在不同的系统资源管理子系统中以层级树(Hierarchy)的方式来组织管理, 每个cgroup可以包含其它子cgroup.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ls -l /sys/fs/cgroup
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;cgroup子系统:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cpu&lt;/li&gt;
&lt;li&gt;cpuacct&lt;/li&gt;
&lt;li&gt;cpuset&lt;/li&gt;
&lt;li&gt;memory&lt;/li&gt;
&lt;li&gt;blkio&lt;/li&gt;
&lt;li&gt;devices&lt;/li&gt;
&lt;li&gt;freezer&lt;/li&gt;
&lt;li&gt;ns&lt;/li&gt;
&lt;li&gt;pid&lt;/li&gt;
&lt;li&gt;net_cls&lt;/li&gt;
&lt;li&gt;net_prio&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;lsns&#34;&gt;lsns&lt;/h1&gt;
&lt;p&gt;查看系统的namespace&lt;/p&gt;
&lt;p&gt;ns类型: mnt, net, ipc, user, pid, uts, cgroup&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$lsns -l
$lsns -t net
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;unshare&#34;&gt;unshare&lt;/h1&gt;
&lt;p&gt;使用非共享的NS运行程序&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 创建unshare 的 ns类型.
$unshare -f -m/-n/-i/-U/-p/-u/-C ... 

// 在net类型的namespace执行sleep命令
$unshare -fn sleep 10
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;nsenter&#34;&gt;nsenter&lt;/h1&gt;
&lt;p&gt;ns enter: Namespace enter.&lt;/p&gt;
&lt;p&gt;nsenter - run program with namespaces of other processes&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nsenter [options] [program [arguments]]

// 指定ns的目标进程，可以通过nsls查看.
$ nsenter -t/--target  

// 进入所有类型的ns.
$ nsenter -a --all ...
// 进入指定类型的ns
$ nsenter -m/-u/-i/-n/-p/-C/-U ...

// 进入指定ns的指定进程
$ nsenter -t &amp;lt;pid&amp;gt; -n ip a

$ nsenter -S/-G/-r/-w ...

$ nsenter -F/--no-fork ...

$ nsenter -Z/--follow-context ...
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Registry</title>
        <link>https://canuxcheng.com/post/cncf_registry/</link>
        <pubDate>Thu, 05 Apr 2018 22:02:31 +0000</pubDate>
        
        <guid>https://canuxcheng.com/post/cncf_registry/</guid>
        <description>&lt;h1 id=&#34;container-registry&#34;&gt;Container Registry&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Docker Hub Registry&lt;/li&gt;
&lt;li&gt;Google Container Registry (gcr.io)&lt;/li&gt;
&lt;li&gt;RedHat Quay Registry (quay.io)&lt;/li&gt;
&lt;li&gt;GitHub Docker Packages (docker.pkg.github.com)(Deprecated)&lt;/li&gt;
&lt;li&gt;GitHub Container Registry (ghcr.io)&lt;/li&gt;
&lt;li&gt;Kubernetes Registry(registry.k8s.io =&amp;gt; k8s.gcr.io)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;harbor&#34;&gt;Harbor&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/goharbor/harbor&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/goharbor/harbor&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Habor是由VMWare中国团队开源的容器镜像仓库, 用于存储和分发docker镜像的registry服务器.&lt;/p&gt;
&lt;p&gt;安装步骤:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;下载并解压安装包, &lt;a class=&#34;link&#34; href=&#34;https://github.com/goharbor/harbor/releases&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/goharbor/harbor/releases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;配置harbor.cfg;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;修改配置:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim harbor.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;运行安装程序:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./install.sh --with-notary --with-clair --with-chartmuseum
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;修改web的port:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ vim /data/harbor/docker-compose.yml
proxy:
  ports:
    - 8080:80 # 默认http是80
    - 4433:443 # 默认https是443
$ vim /data/harbor/harbor.yml
hostname = ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;管理harbor:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cd /data/harbor
# docker-compose down -v 　停止并删除container

&amp;gt; 更新配置
# ./prepare --with-notary --with-clair --with-chartmuseum

&amp;gt; 启动
# docker-compose up -d
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;docker使用harbor&#34;&gt;docker使用harbor&lt;/h2&gt;
&lt;p&gt;Deploy a plain HTTP registry:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/docker/daemon.json
{  &amp;quot;insecure-registries&amp;quot; : [&amp;quot;myregistrydomain.com:5000&amp;quot;]}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use self-signed certificates:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cp your-ca /etc/docker/certs.d/harbor.domain.com/ca.crt
# vim /lib/systemd/system/docker.service
ExecStart=/usr/bin/dockerd --insecure-registry harbor.domain.com:port
# systemctl daemon-reload
# systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;update hosts on docker client:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/hosts ip harbor.domain.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;create user account:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://&amp;lt;ip:port&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;push images to harbor:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker login harbor.domain.com:port
$ docker tag SOURCE_IMAGE[:TAG] harbor.domain.com:port/&amp;lt;project&amp;gt;/IMAGE[:TAG]
$ docker push harbor.domain.com:port/&amp;lt;project&amp;gt;/IMAGE[:TAG]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;pull images from harbor:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker login harbor.domain.com:port
$ docker pull harbor.domain.com:port/&amp;lt;project&amp;gt;/IMAGE[:TAG]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;k8s使用harbor:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;修改存储路径&#34;&gt;修改存储路径&lt;/h2&gt;
&lt;p&gt;默认路径是/data/registry&lt;/p&gt;
&lt;p&gt;停服务:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker-compose down -v
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;修改路径:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ mv /data/* /new/path
$ vim /new/path/harbor/harbor.yml
data_volume: /new/path

$ vim /new/path/harbor/docker-compose.yml
/data =&amp;gt; /new/path
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;启动服务:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd /new/path
$ docker-compose up -d
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;artifactory&#34;&gt;Artifactory&lt;/h2&gt;
</description>
        </item>
        <item>
        <title>Swarm</title>
        <link>https://canuxcheng.com/post/swarm/</link>
        <pubDate>Thu, 05 Apr 2018 22:02:31 +0000</pubDate>
        
        <guid>https://canuxcheng.com/post/swarm/</guid>
        <description>&lt;h1 id=&#34;swarm&#34;&gt;Swarm&lt;/h1&gt;
&lt;p&gt;docker swarm 是 docker内置的容器编排工具。&lt;/p&gt;
&lt;p&gt;从docker1.12开始swarm内置于docker engine.&lt;/p&gt;
&lt;p&gt;swarm mode具有内置kv存储，服务发现，负载均衡，路由网格，动态伸缩，滚动更新，安全传输等功能。&lt;/p&gt;
&lt;p&gt;swarm: 老版本的swarm, 需要kv store, 可以作为独立的container运行, 已废弃, 已经被docker swarm mode 代替.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/docker/swarm&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/docker/swarm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;swarmkit:&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/docker/swarmkit&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/docker/swarmkit&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;swarm命令&#34;&gt;swarm命令&lt;/h1&gt;
&lt;p&gt;创建集群&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker swarm init
--advertise-addr &amp;lt;ip&amp;gt; 多网卡情况下指定manager的ip

docker swarm join --token &amp;lt;token&amp;gt; &amp;lt;host:port&amp;gt;

# 查看token
docker swarm join-token manager   获取添加manager命令
docker swarm join-token worker   获取添加worker命令
docker swarm join-token -q worker

docker swarm leave -f/--force

docker swarm update
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;管理节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker node ls
docker node ls --format &amp;quot;{{.Hostname}}&amp;quot;
​
docker node rm
​
docker node inspect
​
# 查看node上运行的tasks/container
docker node ps -f/--filter NODE
​
# 添加label, node.labels.role=api
docker node update --label-add role=api node1
# 删除label
docker node update --label-rm role node1
​
# 活跃节点
docker node update --availability active node1
# 指定该节点满载,不再分派任务,关闭已有任务并重新分派.
docker node update --availability drain node1
# 已有任务继续运行,不分配新任务.
docker node update --availability pause node1

# 查看所有node和label
docker node ls -q | xargs docker node inspect -f &#39;{{ .ID }} [{{ .Description.Hostname }}]: {{ .Spec.Labels }}&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 相当于docker-compose.yml里面的service.
docker service ls # 列出所有service
​
docker service rm SERVICE
​
docker service inspect SERVICE
​
# 查看service的log
docker service logs -f SERVICE
​
# 查看service的状态,在哪些node上运行,运行状态等
docker service ps SERVICE
​
docker service update
docker service update --image &amp;lt;url:tag&amp;gt; # 根据镜像更新服务
​
docker service scale
​
docker service rollback
​
docker service create
--constraint node.id/node.hostname/node.role/node.labels/engine.labels
--env/-e
--label
--limit-cpu
--limit-memory
--replicas
--restart-condition
--user/--group
--mode global/replicated
--endpoint-mode vip/dnsrr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;stack&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# stack = n*service
# service = n*task(container)
docker stack ls # 列出所有stack

# 查看stack的service
docker stack services &amp;lt;stack&amp;gt;

# 查看stack的task/container
docker stack ps &amp;lt;stack&amp;gt;

docker stack rm STACK

# 根据docker-compose.yml部署应用
docker stack deploy -c/--compose-file &amp;lt;docker-compose.yml&amp;gt; STACK
docker stack deploy --bundle-file &amp;lt;DAB&amp;gt; STACK

# 默认所有node从docker hub pull, 如果是私有镜像，需要加参数
# 需要在manager上docker login private-registry
docker stack deploy --with-registry-auth -c test.yml test

docker stack deploy --resolve-image (always|changed|never) ...

docker stack deploy --orchestrator (swarm|kubernetes|all) ...
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;swarm-compose&#34;&gt;swarm compose&lt;/h1&gt;
&lt;p&gt;其它字段参考Compose， 这里只分析deploy下的字段.&lt;/p&gt;
&lt;p&gt;通过compose文件部署服务.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deploy:
  endpoint_mode:

  // 给service打标签(不作用于container) 
  labels:
    com.examples.key: value

  // 默认mode=replicated, replicas=1.
  mode: replicated
  max_replicas_per_node:
  replicas: 1

  // 部署到匹配的全部node.
  mode: global 
​
  # global和replicated都可以用placement.
  placement:
    preferences: // 只支持spreed.
      - spreed: node.labels.datacenter
    constraints:
      # 多个约束是and关系
      - node.id==...
      - node.hostname==...
      - node.role==...
      - node.role==manager/worker
      - node.platform.os!=windows
      - node.platform.arch==x86
      - engine.labels.&amp;lt;key&amp;gt;==&amp;lt;value&amp;gt;
      # 用户自定义标签
      - node.labels.&amp;lt;key&amp;gt;==&amp;lt;value&amp;gt;
​
  resources:
    limits:
      cpus: &#39;0.5&#39;
      memory: 1G
    reservations:
      cpus: &#39;0.25&#39;
      memory: 20M
​
  restart_policy:
    condition: any(default)/on-failure/none
    delay: 0(default)/5s
    max_attempts: never give up(default)/3
    window: decide immediately(default)/10s
​
  update_config/rollback_config:
    parallelism: 0 (default 0 means all)
    delay: 10s (容器升级间隔时间)
    failure_action: pause(default)/continue/rollback
    monitor: 0s (更新完成后确认成功的时间)
    max_failure_ratio: 更新期间允许的失败率
    order: stop-first(default)/start-first

// 默认endpoint_mode=vip, 支持route mesh, 自动负载均衡和服务发现.
deploy:
  endpoint_mode: vip
ports:
- target: 80
  published: 8080
  // 默认mode=ingress, 也可以改为host.
  mode: ingress
  protocol: tcp/udp
- 8080:80/tcp

// dnsrr 模式
deploy:
  endpoint_mode: dnsrr
  update_config:
    // 如果expose端口，不能start-first, 否则报错no suitable node (host-mode port already in use on 1 node
    order: stop-first 
// 设置iptables规则，外部访问8080通过prerouting做dnat指定目的ip，通过forward转发给container的80.
// iptables-&amp;gt;nat-&amp;gt;prerouting: 
// DNAT tcp -- !docker_gwbridge * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 to:172.18.0.16:80
// iptables-&amp;gt;filter-&amp;gt;forward:
// ACCEPT tcp -- enp5s0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 80
ports:
- target: 80
  published: 8080
  // dnsrr只能用port-&amp;gt;mode=host.
  mode: host
  protocol: tcp/udp
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;swarm-network&#34;&gt;swarm network&lt;/h1&gt;
&lt;p&gt;global模式container:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;eth0: overlay(user define overlay)
eth1: ingress(swarm define ingress)
eth2: docker_gwbridge(swarm define bridge)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;replicate模式container:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;eth0: overlay(user define overlay)
eth1: docker_gwbridge(swarm define bridge)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;overlay: 通过4789/udp跨主机访问其他container, host不能访问overlay的ip，只有container之间通过container-servicename或者container-overlay的ip相互访问.&lt;/p&gt;
&lt;p&gt;overlay 问题: 通过overlay连接，默认15分钟timeout, 所以数据库建议用dnsrr模式; 如果用vip模式，需要修改内核网络参数:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo sysctl -w net.ipv4.tcp_keepalive_time=600 net.ipv4.tcp_keepalive_intvl=60 net.ipv4.tcp_keepalive_probes=3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;vip模式就是访问的虚拟ip,replicated的service如果有多个container,通过servicename访问的就是同一个vip,通过vip解析到背后container的真实overlay-ip(自动负载均衡).&lt;/p&gt;
&lt;p&gt;dnsrr模式就是直接解析container的overlay-ip来访问,如果是replicated的service有多个container,每次访问的就是从dns列表中根据负载均衡算法拿到其中一个overlay-ip.&lt;/p&gt;
&lt;p&gt;ingress network: 是一个特殊的 overlay 网络，用于服务节点间的负载均衡。当任何 Swarm 节点在发布的端口上接收到请求时，它将该请求交给一个名为 IPVS 的模块。IPVS 跟踪参与该服务的所有IP地址，选择其中的一个，并通过 ingress 网络将请求路由到它。&lt;/p&gt;
&lt;p&gt;docker_gwbridge: host和container之间通过ip访问, container能访问host的物理网卡的ip和docker_gwbridge的ip, host也能访问container的docker_gwbridge的ip, 但是container之间不能访问bridge的ip.&lt;/p&gt;
&lt;p&gt;修改默认的docker_gwbridge:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// 对于已存在的，要先删除
$ service docker stop
$ sudo ip link set docker_gwbridge down
$ sudo ip link del dev docker_gwbridge
// 创建swarm之前创建好网络
$ docker network create --subnet &amp;quot;172.18.0.0/16&amp;quot;  --ip-range “172.18.1.0/16” \
--opt com.docker.network.bridge.name=docker_gwbridge \
--opt com.docker.network.bridge.enable_icc=false \
--opt com.docker.network.bridge.enable_ip_masquerade=true \
docker_gwbridge
// 创建swarm
$ docker swarm init
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;endpoint_mode:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;vip: 通过vip这个虚拟ip对外访问，提供负载均衡，不暴露具体的container的ip.&lt;/li&gt;
&lt;li&gt;dnssr: DNS round-robin, 为每个服务设置dns,连接到其中一个具体的contaier的ip.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;swarm-scheduler&#34;&gt;swarm scheduler&lt;/h1&gt;
&lt;p&gt;filter过滤器可以实现特定的容器运行在特定的node上, swarm支持３种策略和6个过滤器.&lt;/p&gt;
&lt;p&gt;swarm strategy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;spread： 默认策略,配置相同的情况下选择容器数量最少的node&lt;/li&gt;
&lt;li&gt;binpack： 尽可能将容器放到一台node上运行。&lt;/li&gt;
&lt;li&gt;random： 直接随机分配&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;swarm node filters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;constraint&lt;/li&gt;
&lt;li&gt;health&lt;/li&gt;
&lt;li&gt;containerslots&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;swarm container-configuration filters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;affinity&lt;/li&gt;
&lt;li&gt;dependency&lt;/li&gt;
&lt;li&gt;port&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Swarm CNI</title>
        <link>https://canuxcheng.com/post/swarm_cni/</link>
        <pubDate>Thu, 05 Apr 2018 22:02:31 +0000</pubDate>
        
        <guid>https://canuxcheng.com/post/swarm_cni/</guid>
        <description>&lt;h1 id=&#34;cni&#34;&gt;CNI&lt;/h1&gt;
&lt;p&gt;CNI: Container Network Intarface&lt;/p&gt;
&lt;h2 id=&#34;单台host上的container通信&#34;&gt;单台host上的container通信&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;none&lt;/li&gt;
&lt;li&gt;host&lt;/li&gt;
&lt;li&gt;bridge&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;多台host之间的container通信&#34;&gt;多台host之间的container通信&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;overlay&lt;/li&gt;
&lt;li&gt;macvlan&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;提供overlaymacvlan的网络服务&#34;&gt;提供overlay/macvlan的网络服务&lt;/h3&gt;
&lt;p&gt;vxlan encapsulated:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;canal&lt;/li&gt;
&lt;li&gt;flannel&lt;/li&gt;
&lt;li&gt;weave&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;bgp unencapsulated:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;calico&lt;/li&gt;
&lt;li&gt;romana&lt;/li&gt;
&lt;li&gt;cilium&lt;/li&gt;
&lt;li&gt;kube-router&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;docker网络管理&#34;&gt;Docker网络管理&lt;/h1&gt;
&lt;p&gt;查看:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker network ls # 查看所有网络
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;默认支持的三种模式:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 默认启动的容器都是桥接(docker0)，重启后容器的ip就变了。
docker run --network bridge  ...
docker run --network host ... # 容器和主机使用相同的ip
docker run --network none ... # 容器不会分配局域网的ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker network create -d &amp;lt;driver&amp;gt; ... [name]
-d/--driver # 默认是bridge, 可选overlay/macvlan
--subnet  # CIDR格式
--gateway
--ip-range
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使用:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker network connect [OPTIONS] NETWORK CONTAINER
$ docker network disconnect [OPTIONS] NETWORK CONTAINER
$ docker run --network [name] --name [container-name] [image:tag]

# 使用自定义bridge网络并指定IP:
docker run --network [name] --ip [ip] ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;opt可用的参数:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;com.docker.network.bridge.name # bridge名字
com.docker.network.bridge.enable_ip_masquerade # iptables:nat, 容器访问外网.
com.docker.network.bridge.enable_icc # iptables:filter, 同一网段容器相互访问.
com.docker.network.bridge.host_binding_ipv4
com.docker.network.driver.mtu
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;bridge网络&#34;&gt;bridge网络&lt;/h1&gt;
&lt;p&gt;bridge网络不能跨主机通信(单网卡情况下), node1上的container不能通过container-hostname/ip访问node2上的container.&lt;/p&gt;
&lt;p&gt;主要用于container访问host并通过host访问外部网络，container能通过ip访问host和局域网中的其他node,或者通过node访问外网。&lt;/p&gt;
&lt;p&gt;host或局域网中的其它机器能通过container-ip(bridge网络)访问container.&lt;/p&gt;
&lt;p&gt;创建:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker network create -d bridge ... [name]

$ docker network create --driver=bridge --gateway=192.168.1.1 --subnet=192.168.1.0/24 --opt com.docker.network.bridge.name=br0 br0

// 定制docker_gwbridge网络
$ docker network create --subnet 172.26.0.0/16 --ip-range 172.26.0.0/16 --gateway 172.26.0.1 \ 
--opt com.docker.network.bridge.name=docker_gwbridge \
--opt com.docker.network.bridge.enable_icc=true \ 
--opt com.docker.network.bridge.enable_ip_masquerade=true \ 
docker_gwbridge
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;docker0:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// dockerd 自动的默认bridge网络，不推荐用于production.

1. 删除docker0:

// 停止
$ ip link set dev docker0 down
$ ifconfig docker0 down

// 删除
$ ip link delete docker0
$ brctl delbr docker0

2. 修改默认网络:
$ vim /etc/docker/daemon.json
{
    &amp;quot;bip&amp;quot;: &amp;quot;192.168.1.5/24&amp;quot;,
    &amp;quot;fixed-cidr&amp;quot;: &amp;quot;192.168.1.5/25&amp;quot;,
    &amp;quot;fixed-cidr-v6&amp;quot;: &amp;quot;2001:db8::/64&amp;quot;,
    &amp;quot;mtu&amp;quot;: 1500,
    &amp;quot;default-gateway&amp;quot;: &amp;quot;10.20.1.1&amp;quot;,
    &amp;quot;default-gateway-v6&amp;quot;: &amp;quot;2001:db8:abcd::89&amp;quot;,
    &amp;quot;dns&amp;quot;: [&amp;quot;10.20.1.2&amp;quot;,&amp;quot;10.20.1.3&amp;quot;]
}
$ service docker restart
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;overlay网络&#34;&gt;overlay网络&lt;/h1&gt;
&lt;p&gt;overlay网络可以实现容器之间的跨主机通信.&lt;/p&gt;
&lt;p&gt;container通过overlay网络实现通信.container能通过service-name/container-ip访问其它container。&lt;/p&gt;
&lt;p&gt;局域网中的node 既不能通过container-servicename也不能通过container-ip(overlay的ip)访问container, 也就是说外部服务只能通过expose port来访问container.&lt;/p&gt;
&lt;p&gt;创建:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker network create -d overlay ... [name]

$ docker network create --attachable --driver=overlay --gateway=172.27.0.1 --subnet=172.27.0.0/24 --ip-range=172.27.0.0/24 --opt com.docker.network.bridge.name=ol0 ol0
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;macvlan&#34;&gt;macvlan&lt;/h1&gt;
&lt;p&gt;macvlan不仅支持在interface上创建，还支持sub-interface(vlan).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ip link set eth1 promisc on  |  
ifconfig eth1 promisc 

docker network create -d macvlan --subnet=192.168.100.0/24 --gateway=192.168.100.1 -o parent=eth1 lan0
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>Swarm CSI</title>
        <link>https://canuxcheng.com/post/swarm_csi/</link>
        <pubDate>Thu, 05 Apr 2018 22:02:31 +0000</pubDate>
        
        <guid>https://canuxcheng.com/post/swarm_csi/</guid>
        <description>&lt;h1 id=&#34;csi&#34;&gt;CSI&lt;/h1&gt;
&lt;p&gt;CSI: Container Storage Interface&lt;/p&gt;
&lt;p&gt;CSI提供容器的数据持久化服务.&lt;/p&gt;
&lt;p&gt;容器管理数据的两种方式：&lt;/p&gt;
&lt;p&gt;数据卷(Volumes)&lt;/p&gt;
&lt;p&gt;挂载主机目录(bind mounts)&lt;/p&gt;
&lt;p&gt;临时文件系统(tmpfs)&lt;/p&gt;
&lt;h2 id=&#34;数据存储原理&#34;&gt;数据存储原理&lt;/h2&gt;
&lt;p&gt;如果container上目录不存在，docker会自动创建&lt;/p&gt;
&lt;p&gt;如果container目录存在且有内容，会被host上的目录覆盖掉，但不会被删除.&lt;/p&gt;
&lt;h2 id=&#34;volumes&#34;&gt;Volumes&lt;/h2&gt;
&lt;p&gt;如果host上目录不存在，docker会自动创建&lt;/p&gt;
&lt;p&gt;volumes是被设计用来持久化数据的，它的生命周期独立于容器.数据卷通过docker volume命令管理的，位于/var/lib/docker/volumes/下面.&lt;/p&gt;
&lt;p&gt;Docker不会在容器被删除后自动删除 数据卷，并且也不存在垃圾回收这样的机制来处理没有任何容器引用的 数据卷。&lt;/p&gt;
&lt;p&gt;创建:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker volume create &amp;lt;volume-name&amp;gt;
$ docker volume rm &amp;lt;volume-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使用：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker run -v/--volume myvolume:/var/lib/app ...
$ docker run --mount source=myvolume,target=/var/lib/app ...
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;bind-mount&#34;&gt;Bind mount&lt;/h2&gt;
&lt;p&gt;如果host上目录不存在会报错，需要提前创建.&lt;/p&gt;
&lt;p&gt;bind mount就是直接将host路径挂在到docker．&lt;/p&gt;
&lt;p&gt;source和target都是文件，即可挂载单个文件.&lt;/p&gt;
&lt;p&gt;使用:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker run -v/--volume /opt/app:/var/lib/app:ro ...
$ docker run --mount type=bind,source=/opt/app,target=/var/lib/app,readonly ...
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;tmpfs&#34;&gt;tmpfs&lt;/h2&gt;
&lt;p&gt;tmpfs是临时文件系统，也叫内存文件系统，就是将数据存在内存上。&lt;/p&gt;
&lt;p&gt;tmpfs只能用于linux, 多个容器也不能共享，容器停止数据就销毁。&lt;/p&gt;
&lt;p&gt;使用&amp;ndash;mount可以指定参数，使用&amp;ndash;tmpfs不能指定参数。&lt;/p&gt;
&lt;p&gt;使用:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker run --mount type=tmpfs,destination=/tmp/app,tmpfs_size=10G tmpfs_mode=1777 ...
$ docker run --tmpfs  ...
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;docker管理volume&#34;&gt;Docker管理volume&lt;/h1&gt;
&lt;p&gt;查看:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker volume ls # 查看所有卷
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建volume:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker volume create my-volume 
# mountpoint: /var/lib/docker/volumes/my-volume/_data.
docker run -v/--volume my-volume:/container/path 
# src=https://canuxcheng.com/var/lib/docker/volumes/my-volume/_data, dest=/container/path
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;指定路径作为volume：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -v /host/path:/container/path ...
# src=https://canuxcheng.com/host/path, dest=/container/path
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;默认随机路径，数据不能持久化：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# src=https://canuxcheng.com/var/lib/docker/volumes/... /_data (on host)
docker run -v /path ....
docker run --mount ...
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        
    </channel>
</rss>
